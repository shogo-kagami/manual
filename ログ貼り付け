user45006@srpv01504:/$ nc -zv 10.121.129.100 10080
Connection to 10.121.129.100 10080 port [tcp/amanda] succeeded!
user45006@srpv01504:/$ nc -zv 10.121.129.100 10081
Connection to 10.121.129.100 10081 port [tcp/kamanda] succeeded!
user45006@srpv01504:/$ nc -zv 10.121.129.100 10082
^C
user45006@srpv01504:/$ nc -zv 10.121.129.100 10083
^C
user45006@srpv01504:/$ nc -zv 10.121.129.100 10084
^C
user45006@srpv01504:/$ nc -zv 10.121.129.100 10085
Connection to 10.121.129.100 10085 port [tcp/*] succeeded!
user45006@srpv01504:/$
user45006@srpv01504:/$
user45006@srpv01504:/$ nc -zv 10.121.128.101 10080
^C
user45006@srpv01504:/$ nc -zv 10.121.128.101 10081
^C
user45006@srpv01504:/$
user45006@srpv01504:/$
user45006@srpv01504:/$ nc -zv 10.121.132.101 10080
^C
user45006@srpv01504:/$ nc -zv 10.121.128.101 10082
^C
user45006@srpv01504:/$ nc -zv 10.121.128.101 10083
^C
user45006@srpv01504:/$ nc -zv 10.121.128.101 10084
^C
user45006@srpv01504:/$ nc -zv 10.121.128.101 10085
Connection to 10.121.128.101 10085 port [tcp/*] succeeded!
user45006@srpv01504:/$
user45006@srpv01504:/$
user45006@srpv01504:/$ nc -zv 10.121.132.101 10080
^C
user45006@srpv01504:/$ nc -zv 10.121.132.101 10081
^C
user45006@srpv01504:/$ nc -zv 10.121.132.101 10082
^C
user45006@srpv01504:/$ nc -zv 10.121.132.101 10083
^C
user45006@srpv01504:/$ nc -zv 10.121.132.101 10084
^C
user45006@srpv01504:/$ nc -zv 10.121.132.101 10085
Connection to 10.121.132.101 10085 port [tcp/*] succeeded!


ーーーーーーーーー
user45006@srpv01505:/$ nc -zv 10.121.129.100 10080
Connection to 10.121.129.100 10080 port [tcp/amanda] succeeded!
user45006@srpv01505:/$ nc -zv 10.121.129.100 10081
Connection to 10.121.129.100 10081 port [tcp/kamanda] succeeded!
user45006@srpv01505:/$ nc -zv 10.121.129.100 10082
^C
user45006@srpv01505:/$ nc -zv 10.121.129.100 10083
^C
user45006@srpv01505:/$ nc -zv 10.121.129.100 10084
^C
user45006@srpv01505:/$ nc -zv 10.121.129.100 10085
Connection to 10.121.129.100 10085 port [tcp/*] succeeded!
user45006@srpv01505:/$
user45006@srpv01505:/$
user45006@srpv01505:/$ nc -zv 10.121.128.101 10080
^C
user45006@srpv01505:/$ nc -zv 10.121.128.101 10081
^C
user45006@srpv01505:/$ nc -zv 10.121.128.101 10082
^C
user45006@srpv01505:/$ nc -zv 10.121.128.101 10083
^C
user45006@srpv01505:/$ nc -zv 10.121.128.101 10084
^C
user45006@srpv01505:/$ nc -zv 10.121.128.101 10085
Connection to 10.121.128.101 10085 port [tcp/*] succeeded!
user45006@srpv01505:/$
user45006@srpv01505:/$
user45006@srpv01505:/$ nc -zv 10.121.132.101 10080
^C
user45006@srpv01505:/$ nc -zv 10.121.132.101 10081
^C
user45006@srpv01505:/$ nc -zv 10.121.132.101 10081
^C
user45006@srpv01505:/$ nc -zv 10.121.132.101 10082
^C
user45006@srpv01505:/$ nc -zv 10.121.132.101 10083
^C
user45006@srpv01505:/$ nc -zv 10.121.132.101 10084
^C
user45006@srpv01505:/$ nc -zv 10.121.132.101 10085
Connection to 10.121.132.101 10085 port [tcp/*] succeeded!

------
user45020@srpv01504:~$ df
Filesystem                                      1K-blocks        Used    Available Use% Mounted on
tmpfs                                             8185856        1240      8184616   1% /run
/dev/sda2                                       104700980    12995160     91705820  13% /
tmpfs                                             8185856          44      8185812   1% /dev/shm
tmpfs                                                5120           0         5120   0% /run/lock
/dev/sda1                                          102156           0       102156   0% /boot/efi
10.120.112.86:/sbint-share                    10737418240  1285838336   9451579904  12% /store/.sysonly_okeFBsTh
srhome-fs1.cm.cluster:/ifs/nfs/home/group045 183897490432 26986285568 148982710272  16% /home
10.120.112.82:/lustre/tenant045              107374182400   166448128 107207734272   1% /lustre
srhome-fs1.cm.cluster:/ifs/nfs/cm_shared     183897490432 26986285568 148982710272  16% /cm/shared
srhome-fs1.cm.cluster:/ifs/nfs/infragroup    183897490432 26986285568 148982710272  16% /infragroup
tmpfs                                             1637168           4      1637164   1% /run/user/14399
tmpfs                                             1637168           4      1637164   1% /run/user/1351


user45020@srpv01504:~$ ls -l /home
total 672
drwxr-x--- 3 user45000 group045 102 Nov 13 16:06 user45000
drwxr-x--- 5 user45001 group045 179 Nov 18 15:22 user45001
drwxr-x--- 3 user45002 group045 102 Nov 13 16:06 user45002
drwxr-x--- 3 user45003 group045 102 Nov 13 16:06 user45003
drwxr-x--- 3 user45004 group045 102 Nov 13 16:06 user45004
drwxr-x--- 5 user45005 group045 179 Nov 18 14:55 user45005
drwxr-x--- 3 user45006 group045 102 Nov 13 16:06 user45006
drwxr-x--- 3 user45007 group045 102 Nov 13 16:06 user45007
drwxr-x--- 3 user45008 group045 102 Nov 13 16:06 user45008
drwxr-x--- 3 user45009 group045 102 Nov 13 16:06 user45009
drwxr-x--- 3 user45010 group045 102 Nov 13 16:06 user45010
drwxr-x--- 3 user45011 group045 102 Nov 13 16:06 user45011
drwxr-x--- 3 user45012 group045 102 Nov 13 16:06 user45012
drwxr-x--- 3 user45013 group045 102 Nov 13 16:06 user45013
drwxr-x--- 3 user45014 group045 102 Nov 13 16:06 user45014
drwxr-x--- 3 user45015 group045 102 Nov 13 16:06 user45015
drwxr-x--- 3 user45016 group045 102 Nov 13 16:06 user45016
drwxr-x--- 3 user45017 group045 102 Nov 13 16:06 user45017
drwxr-x--- 3 user45018 group045 102 Nov 13 16:06 user45018
drwxr-x--- 3 user45019 group045 102 Nov 13 16:06 user45019
drwxr-x--- 6 user45020 group045 255 Nov 18 18:33 user45020


user45020@srpv01504:~$ ls -l /home/user45020
total 32
drwxrwx--- 2 user45020 group045 27 Nov 18 18:00 backup

user45020@srpv01504:/lustre/tools/AdminTools/config$ cat run_worker_conf.py
#!/usr/bin/env python3
###----------------------------------------------------------------###
### シングルRayHead （冗長化未対応）
### 移行作業用設定
### メトリクス送信用 NLB を使用
### JSON 情報取得用の NLB のURL
###----------------------------------------------------------------###
#PROD_SBKK_RELAY01  = 'http://10.121.129.100:10085/get_info'
#PROD_SBKK_RELAY02  = 'http://10.121.129.100:10085/get_info'
#PROD_B2B_RELAY01 = 'http://10.121.129.100:10085/get_info'
#PROD_B2B_RELAY02 = 'http://10.121.129.100:10085/get_info'

###----------------------------------------------------------------###
### マルチテナント & 冗長化対応
### JSON 情報取得用の NLB のURL
###----------------------------------------------------------------###
#PROD_SBKK_RELAY01  = 'http://10.121.128.101:10085/get_info'
#PROD_SBKK_RELAY02  = 'http://10.121.132.101:10085/get_info'

PROD_B2B_RELAY01 = 'http://10.121.128.101:10085/get_info'
PROD_B2B_RELAY02 = 'http://10.121.132.101:10085/get_info'

###----------------------------------------------------------------###
DIVISION_INFO = [
    {'Node': 'srdgx00009', 'URL': PROD_B2B_RELAY01 },
    {'Node': 'srdgx00010', 'URL': PROD_B2B_RELAY02 },
    {'Node': 'srdgx00011', 'URL': PROD_B2B_RELAY01 },
    {'Node': 'srdgx00012', 'URL': PROD_B2B_RELAY02 },
    {'Node': 'srdgx00013', 'URL': PROD_B2B_RELAY01 },
    {'Node': 'srdgx00014', 'URL': PROD_B2B_RELAY02 },
    {'Node': 'srdgx00015', 'URL': PROD_B2B_RELAY01 },
    {'Node': 'srdgx00016', 'URL': PROD_B2B_RELAY02 },
    {'Node': 'srdgx00017', 'URL': PROD_B2B_RELAY01 },
    {'Node': 'srdgx00018', 'URL': PROD_B2B_RELAY02 },
    {'Node': 'srdgx00019', 'URL': PROD_B2B_RELAY01 },
    {'Node': 'srdgx00020', 'URL': PROD_B2B_RELAY02 },
    {'Node': 'srdgx00021', 'URL': PROD_B2B_RELAY01 },
    {'Node': 'srdgx00022', 'URL': PROD_B2B_RELAY02 },
    {'Node': 'srdgx00023', 'URL': PROD_B2B_RELAY01 },
    {'Node': 'srdgx00024', 'URL': PROD_B2B_RELAY02 },
    {'Node': 'srdgx00025', 'URL': PROD_B2B_RELAY01 },
    {'Node': 'srdgx00026', 'URL': PROD_B2B_RELAY02 },
    {'Node': 'srdgx00027', 'URL': PROD_B2B_RELAY01 },
    {'Node': 'srdgx00028', 'URL': PROD_B2B_RELAY02 },
    {'Node': 'srdgx00029', 'URL': PROD_B2B_RELAY01 },
    {'Node': 'srdgx00030', 'URL': PROD_B2B_RELAY02 },
    {'Node': 'srdgx00031', 'URL': PROD_B2B_RELAY01 },
    {'Node': 'srdgx00032', 'URL': PROD_B2B_RELAY02 },
]

###----------------------------------------------------------------###
### << End >>
###----------------------------------------------------------------###


user45020@srpv01504:/cm/shared/apps/slurm/var/cm/statesave/slurm$ ls -l
total 3880
-rw-------  1 slurm slurm   97893 Nov 19 13:07 assoc_mgr_state
-rw-------  1 slurm slurm   97893 Nov 19 13:02 assoc_mgr_state.old
-rw-------  1 slurm slurm   11109 Nov 19 13:07 assoc_usage
-rw-------  1 slurm slurm   11109 Nov 19 13:02 assoc_usage.old
-rw-r--r--  1 slurm slurm       5 Jul 19  2024 clustername
-rw-------  1 slurm slurm       0 Nov 18 10:37 dbd.messages
-rw-------  1 slurm slurm      19 Nov 19 13:07 fed_mgr_state
-rw-------  1 slurm slurm      19 Nov 19 13:02 fed_mgr_state.old
drwx------  9 slurm slurm     203 Nov 19 13:00 hash.0
drwx------ 15 slurm slurm     377 Nov 19 13:02 hash.1
drwx------ 15 slurm slurm     377 Nov 19 13:04 hash.2
drwx------ 12 slurm slurm     290 Nov 19 13:04 hash.3
drwx------ 11 slurm slurm     261 Nov 19 12:59 hash.4
drwx------ 12 slurm slurm     290 Nov 19 13:08 hash.5
drwx------ 13 slurm slurm     319 Nov 19 13:08 hash.6
drwx------ 13 slurm slurm     319 Nov 19 13:06 hash.7
drwx------ 12 slurm slurm     290 Nov 19 13:06 hash.8
drwx------ 12 slurm slurm     290 Nov 19 12:59 hash.9
-rw-------  1 slurm slurm      16 Nov 19 13:11 heartbeat
-rw-------  1 slurm slurm 1164780 Nov 19 13:09 job_state
-rw-------  1 slurm slurm 1164854 Nov 19 13:09 job_state.old
-rw-------  1 slurm slurm      42 Nov 18 10:37 last_config_lite
-rw-------  1 slurm slurm      42 Oct 21 09:03 last_config_lite.old
-rw-------  1 slurm slurm     407 Nov 19 13:07 last_tres
-rw-------  1 slurm slurm     407 Nov 19 13:02 last_tres.old
-rw-------  1 slurm slurm  148499 Nov 19 13:09 node_state
-rw-------  1 slurm slurm  148499 Nov 19 13:09 node_state.old
-rw-------  1 slurm slurm    1170 Nov 19 13:07 part_state
-rw-------  1 slurm slurm    1170 Nov 19 13:02 part_state.old
-rw-------  1 slurm slurm      16 Nov 19 13:07 priority_last_decay_ran
-rw-------  1 slurm slurm      16 Nov 19 13:02 priority_last_decay_ran.old
-rw-------  1 slurm slurm      44 Nov 19 13:07 qos_usage
-rw-------  1 slurm slurm      44 Nov 19 13:02 qos_usage.old
-rw-------  1 slurm slurm      35 Nov 19 13:07 resv_state
-rw-------  1 slurm slurm      35 Nov 19 13:02 resv_state.old
-rw-------  1 slurm slurm      31 Nov 19 13:07 trigger_state
-rw-------  1 slurm slurm      31 Nov 19 13:02 trigger_state.old


user45020@srpv01504:/cm/shared/apps/slurm/var/cm/statesave/slurm$  srun -p 045-partition -w srdgx00009 uname -a
srun: error: Task launch for StepId=2398791.0 failed on node srdgx00009: Communication connection failure
srun: error: Application launch failed: Communication connection failure
srun: Job step aborted


user45020@srpv01505:/store/.sysonly_okeFBsTh/lustre/share/ReleaseCandidate/cksum_transfer$ cat sbint_models.csv
2025-11-18T17:08:23+09:00,/lustre/share/sbint_models/finetuned/release/sbint-2025-08-25/sbint-2025-08-25-70b-sft,417193831941dba748b69f9622d5963e,immediately
2025-11-18T17:08:23+09:00,/lustre/share/sbint_models/guardrail/sarashinaguard-7b-20250508,586865eba73f8f71ebcf4b55bc3d8584,immediately
2025-11-18T17:08:23+09:00,/lustre/share/sbint_models/guardrail/sarashinaguard3-7b-20250822,54937af5e04a3c6547a27c84c9dffafa,immediately
2025-11-18T17:08:23+09:00,/lustre/teams/text_embeddings/experiments/msmd_cse/sarashina2.2-3b/PROTO_sarashina2.2-3B__multistage_20250710_stg2_mix_gecko-recipe-filtered_RRF1_RRF_20_all_forbes_cl_classsification__stg2/hf_checkpoints/checkpoint-770,e08fb6756d04a5394771c7c102c40a1b,immediately


user45020@srpv01505:/lustre/tools/AdminTools/sbin$ ./CksumChecker.sh

INFO : Start : Thu Nov 20 03:03:37 PM JST 2025

INFO :  << /lustre/container_cache/reimei_engine/reimei-engine-llm-serve-gpu:20251119-2.sqsh >>
INFO :       CSVファイルのDate = 2025-11-19T15:53:29+09:00
INFO :      CSVファイルのcksum = 502a3a310cc9b5e456362f0fe9e3c305
INFO :  HISTORYファイルのcksum = 502a3a310cc9b5e456362f0fe9e3c305
INFO :      NFSマウントのcksum = 502a3a310cc9b5e456362f0fe9e3c305
INFO :             実機のcksum = 502a3a310cc9b5e456362f0fe9e3c305

INFO :  << /lustre/share/sbint_models/finetuned/release/sbint-2025-08-25/sbint-2025-08-25-70b-sft >>
INFO :       CSVファイルのDate = 2025-11-18T17:08:23+09:00
INFO :      CSVファイルのcksum = 417193831941dba748b69f9622d5963e
INFO :  HISTORYファイルのcksum = 417193831941dba748b69f9622d5963e
INFO :      NFSマウントのcksum = 417193831941dba748b69f9622d5963e
INFO :             実機のcksum = 417193831941dba748b69f9622d5963e

INFO :  << /lustre/share/sbint_models/guardrail/sarashinaguard-7b-20250508 >>
INFO :       CSVファイルのDate = 2025-11-18T17:08:23+09:00
INFO :      CSVファイルのcksum = 586865eba73f8f71ebcf4b55bc3d8584
INFO : HISTORYファイルにcksumが存在しません : 586865eba73f8f71ebcf4b55bc3d8584
error: not a file or directory: /store/.sysonly_okeFBsTh//lustre/share/sbint_models/guardrail/sarashinaguard-7b-20250508
INFO :      NFSマウントのcksum = --
INFO :             実機のcksum = 227bc609651f929e367c3b2b79e09d5b

INFO :  << /lustre/share/sbint_models/guardrail/sarashinaguard3-7b-20250822 >>
INFO :       CSVファイルのDate = 2025-11-18T17:08:23+09:00
INFO :      CSVファイルのcksum = 54937af5e04a3c6547a27c84c9dffafa
INFO :  HISTORYファイルのcksum = 54937af5e04a3c6547a27c84c9dffafa
INFO :      NFSマウントのcksum = 54937af5e04a3c6547a27c84c9dffafa
INFO :             実機のcksum = 54937af5e04a3c6547a27c84c9dffafa

INFO :  << /lustre/teams/text_embeddings/experiments/msmd_cse/sarashina2.2-3b/PROTO_sarashina2.2-3B__multistage_20250710_stg2_mix_gecko-recipe-filtered_RRF1_RRF_20_all_forbes_cl_classsification__stg2/hf_checkpoints/checkpoint-770 >>
INFO :       CSVファイルのDate = 2025-11-18T17:08:23+09:00
INFO :      CSVファイルのcksum = e08fb6756d04a5394771c7c102c40a1b
INFO : HISTORYファイルにcksumが存在しません : e08fb6756d04a5394771c7c102c40a1b
error: not a file or directory: /lustre/teams/text_embeddings/experiments/msmd_cse/sarashina2.2-3b/PROTO_sarashina2.2-3B__multistage_20250710_stg2_mix_gecko-recipe-filtered_RRF1_RRF_20_all_forbes_cl_classsification__stg2/hf_checkpoints/checkpoint-770
INFO :      NFSマウントのcksum = e08fb6756d04a5394771c7c102c40a1b
INFO :             実機のcksum = --

INFO :   End : Thu Nov 20 03:09:03 PM JST 2025


user45020@srpv01505:/lustre/tools/AdminTools/sbin$ ./CksumChecker.sh

INFO : Start : Thu Nov 20 04:47:54 PM JST 2025

INFO :  << /lustre/container_cache/reimei_engine/reimei-engine-llm-serve-gpu:20251119-2.sqsh >>
INFO :       CSVファイルのDate = 2025-11-19T15:53:29+09:00
INFO :      CSVファイルのcksum = 502a3a310cc9b5e456362f0fe9e3c305
INFO :  HISTORYファイルのcksum = 502a3a310cc9b5e456362f0fe9e3c305
INFO :      NFSマウントのcksum = 502a3a310cc9b5e456362f0fe9e3c305
INFO :             実機のcksum = 502a3a310cc9b5e456362f0fe9e3c305

INFO :  << /lustre/share/sbint_models/finetuned/release/sbint-2025-08-25/sbint-2025-08-25-70b-sft >>
INFO :       CSVファイルのDate = 2025-11-18T17:08:23+09:00
INFO :      CSVファイルのcksum = 417193831941dba748b69f9622d5963e
INFO :  HISTORYファイルのcksum = 417193831941dba748b69f9622d5963e
INFO :      NFSマウントのcksum = 417193831941dba748b69f9622d5963e
INFO :             実機のcksum = 417193831941dba748b69f9622d5963e

INFO :  << /lustre/share/sbint_models/guardrail/sarashinaguard3-7b-20250822 >>
INFO :       CSVファイルのDate = 2025-11-18T17:08:23+09:00
INFO :      CSVファイルのcksum = 54937af5e04a3c6547a27c84c9dffafa
INFO :  HISTORYファイルのcksum = 54937af5e04a3c6547a27c84c9dffafa
INFO :      NFSマウントのcksum = 54937af5e04a3c6547a27c84c9dffafa
INFO :             実機のcksum = 54937af5e04a3c6547a27c84c9dffafa

INFO :  << /lustre/teams/text_embeddings/experiments/msmd_cse/sarashina2.2-3b/PROTO_sarashina2.2-3B__multistage_20250710_stg2_mix_gecko-recipe-filtered_RRF1_RRF_20_all_forbes_cl_classsification__stg2/hf_checkpoints/checkpoint-770 >>
INFO :       CSVファイルのDate = 2025-11-18T17:08:23+09:00
INFO :      CSVファイルのcksum = e08fb6756d04a5394771c7c102c40a1b
INFO :  HISTORYファイルのcksum = e08fb6756d04a5394771c7c102c40a1b
INFO :      NFSマウントのcksum = e08fb6756d04a5394771c7c102c40a1b
INFO :             実機のcksum = e08fb6756d04a5394771c7c102c40a1b

INFO :   End : Thu Nov 20 04:54:09 PM JST 2025


user45020@srpv01505:/lustre/tools/AdminTools/sbin$ ./CksumChecker.sh

INFO : Start : Thu Nov 20 05:30:04 PM JST 2025

INFO :  << /lustre/container_cache/reimei_engine/reimei-engine-llm-serve-gpu:20251119-2.sqsh >>
INFO :       CSVファイルのDate = 2025-11-19T15:53:29+09:00
INFO :      CSVファイルのcksum = 502a3a310cc9b5e456362f0fe9e3c305
INFO :  HISTORYファイルのcksum = 502a3a310cc9b5e456362f0fe9e3c305
INFO :      NFSマウントのcksum = 502a3a310cc9b5e456362f0fe9e3c305
INFO :             実機のcksum = 502a3a310cc9b5e456362f0fe9e3c305

INFO :  << /lustre/share/sbint_models/guardrail/sarashinaguard-7b-20250508 >>
INFO :       CSVファイルのDate = 2025-11-20T17:21:03+09:00
INFO :      CSVファイルのcksum = 586865eba73f8f71ebcf4b55bc3d8584
INFO :  HISTORYファイルのcksum = 586865eba73f8f71ebcf4b55bc3d8584
INFO :      NFSマウントのcksum = 586865eba73f8f71ebcf4b55bc3d8584
INFO :             実機のcksum = 586865eba73f8f71ebcf4b55bc3d8584

INFO :   End : Thu Nov 20 05:32:15 PM JST 2025

user45020@srpv01504:~$ sinfo -a | grep 00011
defq                     up   infinite     19  drain srdgx[00011-00012,00039,00046,00055,00068,00116,00128-00129,00137,00144,00146,00158,00176,00189,00209,00236,00255],srpp00045
user45020@srpv01504:~$
user45020@srpv01504:~$
user45020@srpv01504:~$ sinfo -a | grep 00025
defq                     up   infinite      5 drain* srdgx[00025-00026,00239],srpp[00029,00046]

user45020@srpv01504:~$ sinfo
PARTITION     AVAIL  TIMELIMIT  NODES  STATE NODELIST
045-partition    up   infinite     20    mix srdgx[00009-00010,00013-00024,00027-00032]


user45020@srdgx00010:~$ df -h
Filesystem                                                                   Size  Used Avail Use% Mounted on
tmpfs                                                                       1008G  4.8M 1008G   1% /run
/dev/md0                                                                     1.8T   15G  1.7T   1% /
tmpfs                                                                       1008G   16K 1008G   1% /dev/shm
tmpfs                                                                        5.0M     0  5.0M   0% /run/lock
tmpfs                                                                        4.0M     0  4.0M   0% /sys/fs/cgroup
/dev/nvme3n1p1                                                               511M   11M  501M   3% /boot/efi
/dev/md1                                                                      28T   25G   27T   1% /raid
srhome-fs1.cm.cluster:/ifs/nfs/cm_shared                                     172T   27T  138T  17% /cm/shared
srhome-fs1.cm.cluster:/ifs/nfs/infragroup                                    172T   27T  138T  17% /infragroup
srhome-fs1.cm.cluster:/ifs/nfs/home/group045                                 172T   27T  138T  17% /home
100.127.5.1@o2ib:100.127.5.5@o2ib:100.127.5.2@o2ib:100.127.5.6@o2ib:/lustre  100T  1.1T   99T   2% /lustre
tmpfs                                                                        202G  4.0K  202G   1% /run/user/14399

user45020@srdgx00009:~$ df -h
Filesystem                                                                   Size  Used Avail Use% Mounted on
tmpfs                                                                       1008G  4.8M 1008G   1% /run
/dev/md0                                                                     1.8T   15G  1.7T   1% /
tmpfs                                                                       1008G  144M 1008G   1% /dev/shm
tmpfs                                                                        5.0M     0  5.0M   0% /run/lock
tmpfs                                                                        4.0M     0  4.0M   0% /sys/fs/cgroup
/dev/nvme0n1p1                                                               511M   11M  501M   3% /boot/efi
/dev/md1                                                                      28T   37G   27T   1% /raid
srhome-fs1.cm.cluster:/ifs/nfs/infragroup                                    172T   27T  138T  17% /infragroup
srhome-fs1.cm.cluster:/ifs/nfs/home/group045                                 172T   27T  138T  17% /home
srhome-fs1.cm.cluster:/ifs/nfs/cm_shared                                     172T   27T  138T  17% /cm/shared
100.127.5.1@o2ib:100.127.5.5@o2ib:100.127.5.2@o2ib:100.127.5.6@o2ib:/lustre  100T  1.1T   99T   2% /lustre


[ec2-user@ip-10-121-128-124 ~]$ df -h
Filesystem        Size  Used Avail Use% Mounted on
devtmpfs          4.0M     0  4.0M   0% /dev
tmpfs             3.8G     0  3.8G   0% /dev/shm
tmpfs             1.6G  720K  1.6G   1% /run
/dev/nvme0n1p1    100G  4.6G   96G   5% /
tmpfs             3.8G     0  3.8G   0% /tmp
/dev/nvme0n1p128   10M  1.3M  8.7M  13% /boot/efi
tmpfs             778M     0  778M   0% /run/user/1000

[ec2-user@ip-10-121-132-114 ~]$ df -h
Filesystem        Size  Used Avail Use% Mounted on
devtmpfs          4.0M     0  4.0M   0% /dev
tmpfs             3.8G     0  3.8G   0% /dev/shm
tmpfs             1.6G  720K  1.6G   1% /run
/dev/nvme0n1p1    100G  4.6G   96G   5% /
tmpfs             3.8G     0  3.8G   0% /tmp
/dev/nvme0n1p128   10M  1.3M  8.7M  13% /boot/efi
tmpfs             778M     0  778M   0% /run/user/1000


You are required to change your password immediately (password expired).
Last login: Thu Dec 18 11:25:17 2025 from 10.120.64.252
WARNING: Your password has expired.
You must change your password now and login again!
(current) LDAP Password:


user45020@fcpv00286:/lustre/tools/PutDataToAWS/SBintTools/Operations$ squeue
             JOBID          PARTITION                   NAME           USER ST       TIME  NODES NODELIST(REASON)
           1155438      045-partition manual-drain_fcdgx0006      user45020 PD       0:00      1 (ReqNodeNotAvail, UnavailableNodes:fcdgx00067)
