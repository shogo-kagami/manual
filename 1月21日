
prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~ (main)
$ aws configure sso
SSO session name (Recommended): prod
Attempting to open your default browser.
If the browser does not open, open the following URL:

https://oidc.ap-northeast-1.amazonaws.com/authorize?response_type=code&client_id=5V_8Uzhxuzrp-VLg2lu6IGFwLW5vcnRoZWFzdC0x&redirect_uri=http%3A%2F%2F127.0.0.1%3A56806%2Foauth%2Fcallback&state=b6b8d5aa-c7da-4069-9a4b-9c5bd25e8574&code_challenge_method=S256&scopes=sso%3Aaccount%3Aaccess&code_challenge=PDHutn7q6oPfeG6RJgnexs2AG2kFabTqbVIZcBqnRik
There are 2 AWS accounts available to you.
Using the account ID 050451382758
The only role available to you is: AWSAdministratorAccess
Using the role name "AWSAdministratorAccess"
Default client Region [None]:
CLI default output format (json if not specified) [None]:
Profile name [AWSAdministratorAccess-050451382758]:
To use this profile, specify the profile name using --profile, as shown:

aws sts get-caller-identity --profile AWSAdministratorAccess-050451382758

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~ (main)
$ cd ~

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~ (main)
$ pwd
/c/Users/prod.shogo.kagami

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~ (main)
$ mkdir deploy_20260121

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~ (main)
$ cd deploy_20260121

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121 (main)
$ pwd
/c/Users/prod.shogo.kagami/deploy_20260121

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121 (main)
$ git clone https://github.com/sbintuitions/reimei_RAG -b b2b-demo-gpu-prod-release-20260121-1
Cloning into 'reimei_RAG'...
remote: Enumerating objects: 30829, done.
remote: Counting objects: 100% (802/802), done.
remote: Compressing objects: 100% (258/258), done.
remote: Total 30829 (delta 693), reused 545 (delta 544), pack-reused 30027 (from 4)
Receiving objects: 100% (30829/30829), 66.16 MiB | 20.69 MiB/s, done.
Resolving deltas: 100% (20451/20451), done.
warning: refs/tags/b2b-demo-gpu-prod-release-20260121-1 24a5a6ee7b2f0d9fce1a79066c8488344c48197c is not a commit!
Note: switching to 'c337cd2210d43520e0fbdd8335c51a19631744bb'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

Updating files: 100% (1383/1383), done.

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121 (main)
$ export AWS_PROFILE=AWSAdministratorAccess-050451382758

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121 (main)
$ cd reimei_RAG/terraform/environments_b2b_demo_gpu/prod

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((b2b-demo-gpu-prod-release-20260121-1))
$ pwd
/c/Users/prod.shogo.kagami/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((b2b-demo-gpu-prod-release-20260121-1))
$ terraform init
Initializing the backend...
Initializing modules...
- gpu_instance in ..\..\modules\gpu-ec2-instance
- s3_tfstate in ..\..\modules\s3-bucket-tfstate
Initializing provider plugins...
- Reusing previous version of hashicorp/cloudinit from the dependency lock file
- Reusing previous version of hashicorp/local from the dependency lock file
- Reusing previous version of hashicorp/aws from the dependency lock file
- Installing hashicorp/cloudinit v2.3.7...
- Installed hashicorp/cloudinit v2.3.7 (signed by HashiCorp)
- Installing hashicorp/local v2.6.1...
- Installed hashicorp/local v2.6.1 (signed by HashiCorp)
- Installing hashicorp/aws v6.24.0...
- Installed hashicorp/aws v6.24.0 (signed by HashiCorp)

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((b2b-demo-gpu-prod-release-20260121-1))
$ terraform plan -no-color | tee ~/log/$(date +%Y%m%d)_tfplan.txt
data.aws_subnet.reimei_rag_vpc_subnet_private02: Reading...
data.aws_subnet.reimei_rag_vpc_subnet_private01: Reading...
data.aws_ecr_repository.gpu: Reading...
data.aws_vpc.reimei_rag_main_vpc: Reading...
data.aws_subnet.reimei_rag_vpc_subnet_private03: Reading...
module.s3_tfstate.data.aws_caller_identity.current: Reading...
module.s3_tfstate.data.aws_caller_identity.current: Read complete after 0s [id=050451382758]
data.aws_subnet.reimei_rag_vpc_subnet_private01: Read complete after 1s [id=subnet-0fcce540a819b1bb3]
data.aws_subnet.reimei_rag_vpc_subnet_private02: Read complete after 1s [id=subnet-07c0c0990467fe98d]
data.aws_subnet.reimei_rag_vpc_subnet_private03: Read complete after 1s [id=subnet-0f36d0c08cda116c4]
data.aws_ecr_repository.gpu: Read complete after 1s [id=sbint-services/reimei-engine-ray-stg/llm-serve-gpu]
data.aws_vpc.reimei_rag_main_vpc: Read complete after 1s [id=vpc-07758e1658c4c2149]

Terraform used the selected providers to generate the following execution
plan. Resource actions are indicated with the following symbols:
  + create
 <= read (data resources)

Terraform will perform the following actions:

  # data.cloudinit_config.init will be read during apply
  # (depends on a resource or a module with changes pending)
 <= data "cloudinit_config" "init" {
      + base64_encode = false
      + boundary      = (known after apply)
      + gzip          = false
      + id            = (known after apply)
      + rendered      = (known after apply)

      + part {
          + content      = <<-EOT
                #cloud-config
                cloud_final_modules:
                - [ scripts-user, always ]
            EOT
          + content_type = "text/cloud-config"
        }
      + part {
          + content      = <<-EOT
                #!/bin/bash
                set -eux -o pipefail

                # Create config.yaml
                cat > /home/ec2-user/config.yaml << YAML
                http_options:
                  host: 0.0.0.0
                  port: 8000
                  keep_alive_timeout_s: 200

                # https://docs.ray.io/en/latest/serve/monitoring.html#configure-serve-logging
                logging_config:
                  log_level: DEBUG
                  logs_dir: null # defaults to ray temp dir
                  encoding: JSON
                  enable_access_log: true

                applications:
                  - name: llm-inference-app
                    route_prefix: /
                    import_path: llm_serve.app.builder:build_openai_app
                    runtime_env:
                      env_vars:
                        RAY_LOG_TO_DRIVER: '1'
                        RAY_LOG_TO_DRIVER_EVENT_LEVEL: INFO
                        VLLM_NO_USAGE_STATS: '1'
                    args:
                      deployment_options:
                        max_ongoing_requests: 64
                        name: llm-server-router
                        num_replicas: 8
                        ray_actor_options:
                          num_cpus: 0.1
                      llm_server_configs:
                        - deployment_options:
                            max_ongoing_requests: 128
                            name: sarashinachat-7b
                            num_replicas: 1
                            ray_actor_options:
                              num_cpus: 0.1
                              num_gpus: 0.4
                          engine: vllm
                          engine_kwargs:
                            model: /config/models/7b-2512-sft-dpo
                            override_generation_config:
                              repetition_penalty: 1.05
                              temperature: 0.7
                              top_p: 0.9
                            served_model_name:
                              - sbint-instruct-2025-11-25:70b-sft
                              - sarashina2-mini
                              - sarashina-mini
                              - tmp-new-70b-sft-safety
                            gpu_memory_utilization: 0.4
                            max_model_len: 16384
                            num_gpu_blocks_override: 1024
                            enforce_eager: true
                          server_kwargs:
                            enable_auto_tool_choice: true
                            tool_call_parser: sarashina2_json
                        - deployment_options:
                            max_ongoing_requests: 128
                            name: sarashinaguard-7b
                            num_replicas: 1
                            ray_actor_options:
                              num_cpus: 0.1
                              num_gpus: 0.4
                          engine: vllm
                          engine_kwargs:
                            model: /config/models/sarashinaguard-7b-20250508
                            served_model_name:
                              - sarashinaguard-7b-20250508
                              - sarashinaguard-v2.0.1
                            gpu_memory_utilization: 0.4
                            max_model_len: 16384
                            num_gpu_blocks_override: 1024
                            enforce_eager: true
                        - deployment_options:
                            max_ongoing_requests: 128
                            name: sarashinaembedding-1b
                            num_replicas: 1
                            ray_actor_options:
                              num_cpus: 0.1
                              num_gpus: 0.1
                          engine: vllm
                          engine_kwargs:
                            model: /config/models/sarashina-embedding-v2-1b
                            served_model_name:
                              - sarashina-embedding-v2-1b
                            gpu_memory_utilization: 0.1
                            max_model_len: 8192
                            num_gpu_blocks_override: 512
                            enforce_eager: true
                        - deployment_options:
                            max_ongoing_requests: 128
                            name: sarashinareranker-0.5b
                            num_replicas: 1
                            ray_actor_options:
                              num_cpus: 0.1
                              num_gpus: 0.1
                          engine: vllm
                          engine_kwargs:
                            model: /config/models/sarashina2.2-0.5b-reranker-toy-0523
                            served_model_name:
                              - sarashina2.2-0.5b-reranker-toy-0523
                            gpu_memory_utilization: 0.1
                            max_model_len: 8192
                            num_gpu_blocks_override: 512
                            enforce_eager: true

                YAML

                aws s3 cp --recursive s3://reimei-b2b-demo-gpu-prd-model-storage /home/ec2-user/models
                ls -la /home/ec2-user/models
                aws ecr get-login-password --region ap-northeast-1 | docker login --username AWS --password-stdin 050451382758.dkr.ecr.ap-northeast-1.amazonaws.com/sbint-services/reimei-engine-ray-stg/llm-serve-gpu
                docker run --gpus all --rm --entrypoint '' -v /home/ec2-user:/config --network host -e MAX_PER_REPLICA_RETRY_COUNT=100 -e MAX_DEPLOYMENT_CONSTRUCTOR_RETRY_COUNT=100 -e TZ=UTC \
                  050451382758.dkr.ecr.ap-northeast-1.amazonaws.com/sbint-services/reimei-engine-ray-stg/llm-serve-gpu:20251224-1 serve run /config/config.yaml
            EOT
          + content_type = "text/x-shellscript"
          + filename     = "setup_ec2.sh"
        }
    }

  # aws_lb.nlb will be created
  + resource "aws_lb" "nlb" {
      + arn                                                          = (known after apply)
      + arn_suffix                                                   = (known after apply)
      + dns_name                                                     = (known after apply)
      + dns_record_client_routing_policy                             = "any_availability_zone"
      + enable_cross_zone_load_balancing                             = false
      + enable_deletion_protection                                   = false
      + enable_zonal_shift                                           = false
      + enforce_security_group_inbound_rules_on_private_link_traffic = (known after apply)
      + id                                                           = (known after apply)
      + internal                                                     = true
      + ip_address_type                                              = (known after apply)
      + load_balancer_type                                           = "network"
      + name                                                         = "reimei-b2b-demo-gpu-prd-nlb"
      + name_prefix                                                  = (known after apply)
      + region                                                       = "ap-northeast-1"
      + secondary_ips_auto_assigned_per_subnet                       = (known after apply)
      + security_groups                                              = (known after apply)
      + subnets                                                      = [
          + "subnet-07c0c0990467fe98d",
          + "subnet-0f36d0c08cda116c4",
          + "subnet-0fcce540a819b1bb3",
        ]
      + tags_all                                                     = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + vpc_id                                                       = (known after apply)
      + zone_id                                                      = (known after apply)

      + subnet_mapping (known after apply)
    }

  # aws_lb_listener.nlb will be created
  + resource "aws_lb_listener" "nlb" {
      + arn                                                                   = (known after apply)
      + id                                                                    = (known after apply)
      + load_balancer_arn                                                     = (known after apply)
      + port                                                                  = 8000
      + protocol                                                              = "TCP"
      + region                                                                = "ap-northeast-1"
      + routing_http_request_x_amzn_mtls_clientcert_header_name               = (known after apply)
      + routing_http_request_x_amzn_mtls_clientcert_issuer_header_name        = (known after apply)
      + routing_http_request_x_amzn_mtls_clientcert_leaf_header_name          = (known after apply)
      + routing_http_request_x_amzn_mtls_clientcert_serial_number_header_name = (known after apply)
      + routing_http_request_x_amzn_mtls_clientcert_subject_header_name       = (known after apply)
      + routing_http_request_x_amzn_mtls_clientcert_validity_header_name      = (known after apply)
      + routing_http_request_x_amzn_tls_cipher_suite_header_name              = (known after apply)
      + routing_http_request_x_amzn_tls_version_header_name                   = (known after apply)
      + routing_http_response_access_control_allow_credentials_header_value   = (known after apply)
      + routing_http_response_access_control_allow_headers_header_value       = (known after apply)
      + routing_http_response_access_control_allow_methods_header_value       = (known after apply)
      + routing_http_response_access_control_allow_origin_header_value        = (known after apply)
      + routing_http_response_access_control_expose_headers_header_value      = (known after apply)
      + routing_http_response_access_control_max_age_header_value             = (known after apply)
      + routing_http_response_content_security_policy_header_value            = (known after apply)
      + routing_http_response_server_enabled                                  = (known after apply)
      + routing_http_response_strict_transport_security_header_value          = (known after apply)
      + routing_http_response_x_content_type_options_header_value             = (known after apply)
      + routing_http_response_x_frame_options_header_value                    = (known after apply)
      + ssl_policy                                                            = (known after apply)
      + tags_all                                                              = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + tcp_idle_timeout_seconds                                              = (known after apply)

      + default_action {
          + order            = (known after apply)
          + target_group_arn = (known after apply)
          + type             = "forward"
        }

      + mutual_authentication (known after apply)
    }

  # aws_lb_target_group.nlb will be created
  + resource "aws_lb_target_group" "nlb" {
      + arn                                = (known after apply)
      + arn_suffix                         = (known after apply)
      + connection_termination             = (known after apply)
      + deregistration_delay               = "300"
      + id                                 = (known after apply)
      + ip_address_type                    = (known after apply)
      + lambda_multi_value_headers_enabled = false
      + load_balancer_arns                 = (known after apply)
      + load_balancing_algorithm_type      = (known after apply)
      + load_balancing_anomaly_mitigation  = (known after apply)
      + load_balancing_cross_zone_enabled  = (known after apply)
      + name                               = "reimei-b2b-demo-gpu-prd-tg"
      + name_prefix                        = (known after apply)
      + port                               = 8000
      + preserve_client_ip                 = (known after apply)
      + protocol                           = "TCP"
      + protocol_version                   = (known after apply)
      + proxy_protocol_v2                  = false
      + region                             = "ap-northeast-1"
      + slow_start                         = 0
      + tags_all                           = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + target_type                        = "instance"
      + vpc_id                             = "vpc-07758e1658c4c2149"

      + health_check {
          + enabled             = true
          + healthy_threshold   = 2
          + interval            = 30
          + matcher             = "200"
          + path                = "/health"
          + port                = "traffic-port"
          + protocol            = "HTTP"
          + timeout             = 5
          + unhealthy_threshold = 3
        }

      + stickiness (known after apply)

      + target_failover (known after apply)

      + target_group_health (known after apply)

      + target_health_state (known after apply)
    }

  # aws_lb_target_group_attachment.nlb will be created
  + resource "aws_lb_target_group_attachment" "nlb" {
      + id               = (known after apply)
      + port             = 8000
      + region           = "ap-northeast-1"
      + target_group_arn = (known after apply)
      + target_id        = (known after apply)
    }

  # aws_s3_bucket.model_storage will be created
  + resource "aws_s3_bucket" "model_storage" {
      + acceleration_status         = (known after apply)
      + acl                         = (known after apply)
      + arn                         = (known after apply)
      + bucket                      = "reimei-b2b-demo-gpu-prd-model-storage"
      + bucket_domain_name          = (known after apply)
      + bucket_prefix               = (known after apply)
      + bucket_region               = (known after apply)
      + bucket_regional_domain_name = (known after apply)
      + force_destroy               = false
      + hosted_zone_id              = (known after apply)
      + id                          = (known after apply)
      + object_lock_enabled         = (known after apply)
      + policy                      = (known after apply)
      + region                      = "ap-northeast-1"
      + request_payer               = (known after apply)
      + tags_all                    = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + website_domain              = (known after apply)
      + website_endpoint            = (known after apply)

      + cors_rule (known after apply)

      + grant (known after apply)

      + lifecycle_rule (known after apply)

      + logging (known after apply)

      + object_lock_configuration (known after apply)

      + replication_configuration (known after apply)

      + server_side_encryption_configuration (known after apply)

      + versioning (known after apply)

      + website (known after apply)
    }

  # aws_security_group.gpu_instance will be created
  + resource "aws_security_group" "gpu_instance" {
      + arn                    = (known after apply)
      + description            = "Security group for demo inference GPU instances"
      + egress                 = [
          + {
              + cidr_blocks      = [
                  + "0.0.0.0/0",
                ]
              + from_port        = 0
              + ipv6_cidr_blocks = []
              + prefix_list_ids  = []
              + protocol         = "-1"
              + security_groups  = []
              + self             = false
              + to_port          = 0
                # (1 unchanged attribute hidden)
            },
        ]
      + id                     = (known after apply)
      + ingress                = [
          + {
              + cidr_blocks      = [
                  + "10.31.140.0/22",
                  + "10.31.144.0/22",
                  + "10.31.148.0/22",
                ]
              + from_port        = 8000
              + ipv6_cidr_blocks = []
              + prefix_list_ids  = []
              + protocol         = "tcp"
              + security_groups  = []
              + self             = false
              + to_port          = 8000
                # (1 unchanged attribute hidden)
            },
        ]
      + name                   = "reimei-b2b-demo-gpu-prd-gpu-instance-sg"
      + name_prefix            = (known after apply)
      + owner_id               = (known after apply)
      + region                 = "ap-northeast-1"
      + revoke_rules_on_delete = false
      + tags                   = {
          + "Name" = "reimei-b2b-demo-gpu-prd-gpu-instance-sg"
        }
      + tags_all               = {
          + "Env"         = "prd"
          + "Name"        = "reimei-b2b-demo-gpu-prd-gpu-instance-sg"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + vpc_id                 = "vpc-07758e1658c4c2149"
    }

  # module.gpu_instance.aws_cloudwatch_log_group.ec2_logs will be created
  + resource "aws_cloudwatch_log_group" "ec2_logs" {
      + arn               = (known after apply)
      + id                = (known after apply)
      + log_group_class   = (known after apply)
      + name              = "/ec2/reimei-b2b-demo-gpu-prd/gpu-inference"
      + name_prefix       = (known after apply)
      + region            = "ap-northeast-1"
      + retention_in_days = 30
      + skip_destroy      = false
      + tags_all          = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
    }

  # module.gpu_instance.aws_iam_instance_profile.ec2_profile will be created
  + resource "aws_iam_instance_profile" "ec2_profile" {
      + arn         = (known after apply)
      + create_date = (known after apply)
      + id          = (known after apply)
      + name        = "reimei-b2b-demo-gpu-prd-gpu-inference-instance-profile"
      + name_prefix = (known after apply)
      + path        = "/"
      + role        = "reimei-b2b-demo-gpu-prd-gpu-inference-role"
      + tags_all    = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + unique_id   = (known after apply)
    }

  # module.gpu_instance.aws_iam_role.ec2_role will be created
  + resource "aws_iam_role" "ec2_role" {
      + arn                   = (known after apply)
      + assume_role_policy    = jsonencode(
            {
              + Statement = [
                  + {
                      + Action    = "sts:AssumeRole"
                      + Effect    = "Allow"
                      + Principal = {
                          + Service = "ec2.amazonaws.com"
                        }
                    },
                ]
              + Version   = "2012-10-17"
            }
        )
      + create_date           = (known after apply)
      + force_detach_policies = false
      + id                    = (known after apply)
      + managed_policy_arns   = (known after apply)
      + max_session_duration  = 3600
      + name                  = "reimei-b2b-demo-gpu-prd-gpu-inference-role"
      + name_prefix           = (known after apply)
      + path                  = "/"
      + tags_all              = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + unique_id             = (known after apply)

      + inline_policy (known after apply)
    }

  # module.gpu_instance.aws_iam_role_policy_attachment.custom_policies["arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly"] will be created
  + resource "aws_iam_role_policy_attachment" "custom_policies" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly"
      + role       = "reimei-b2b-demo-gpu-prd-gpu-inference-role"
    }

  # module.gpu_instance.aws_iam_role_policy_attachment.custom_policies["arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"] will be created
  + resource "aws_iam_role_policy_attachment" "custom_policies" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
      + role       = "reimei-b2b-demo-gpu-prd-gpu-inference-role"
    }

  # module.gpu_instance.aws_iam_role_policy_attachment.cw_agent_managed will be created
  + resource "aws_iam_role_policy_attachment" "cw_agent_managed" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy"
      + role       = "reimei-b2b-demo-gpu-prd-gpu-inference-role"
    }

  # module.gpu_instance.aws_iam_role_policy_attachment.ssm_managed will be created
  + resource "aws_iam_role_policy_attachment" "ssm_managed" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
      + role       = "reimei-b2b-demo-gpu-prd-gpu-inference-role"
    }

  # module.gpu_instance.aws_instance.this will be created
  + resource "aws_instance" "this" {
      + ami                                  = "ami-0591d1602f530f886"
      + arn                                  = (known after apply)
      + associate_public_ip_address          = (known after apply)
      + availability_zone                    = (known after apply)
      + disable_api_stop                     = (known after apply)
      + disable_api_termination              = (known after apply)
      + ebs_optimized                        = (known after apply)
      + enable_primary_ipv6                  = (known after apply)
      + force_destroy                        = false
      + get_password_data                    = false
      + host_id                              = (known after apply)
      + host_resource_group_arn              = (known after apply)
      + iam_instance_profile                 = "reimei-b2b-demo-gpu-prd-gpu-inference-instance-profile"
      + id                                   = (known after apply)
      + instance_initiated_shutdown_behavior = (known after apply)
      + instance_lifecycle                   = (known after apply)
      + instance_state                       = (known after apply)
      + instance_type                        = "g6e.xlarge"
      + ipv6_address_count                   = (known after apply)
      + ipv6_addresses                       = (known after apply)
      + key_name                             = (known after apply)
      + monitoring                           = (known after apply)
      + outpost_arn                          = (known after apply)
      + password_data                        = (known after apply)
      + placement_group                      = (known after apply)
      + placement_group_id                   = (known after apply)
      + placement_partition_number           = (known after apply)
      + primary_network_interface_id         = (known after apply)
      + private_dns                          = (known after apply)
      + private_ip                           = "10.31.140.50"
      + public_dns                           = (known after apply)
      + public_ip                            = (known after apply)
      + region                               = "ap-northeast-1"
      + secondary_private_ips                = (known after apply)
      + security_groups                      = (known after apply)
      + source_dest_check                    = true
      + spot_instance_request_id             = (known after apply)
      + subnet_id                            = "subnet-0fcce540a819b1bb3"
      + tags                                 = {
          + "Name" = "reimei-b2b-demo-gpu-prd-gpu-inference"
        }
      + tags_all                             = {
          + "Env"         = "prd"
          + "Name"        = "reimei-b2b-demo-gpu-prd-gpu-inference"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + tenancy                              = (known after apply)
      + user_data                            = (known after apply)
      + user_data_base64                     = (known after apply)
      + user_data_replace_on_change          = false
      + vpc_security_group_ids               = (known after apply)

      + capacity_reservation_specification (known after apply)

      + cpu_options (known after apply)

      + ebs_block_device (known after apply)

      + enclave_options (known after apply)

      + ephemeral_block_device (known after apply)

      + instance_market_options (known after apply)

      + maintenance_options (known after apply)

      + metadata_options (known after apply)

      + network_interface (known after apply)

      + primary_network_interface (known after apply)

      + private_dns_name_options (known after apply)

      + root_block_device {
          + delete_on_termination = true
          + device_name           = (known after apply)
          + encrypted             = (known after apply)
          + iops                  = 3000
          + kms_key_id            = (known after apply)
          + tags_all              = (known after apply)
          + throughput            = 125
          + volume_id             = (known after apply)
          + volume_size           = 120
          + volume_type           = "gp3"
        }
    }

  # module.s3_tfstate.aws_s3_bucket.tfstate_bucket will be created
  + resource "aws_s3_bucket" "tfstate_bucket" {
      + acceleration_status         = (known after apply)
      + acl                         = (known after apply)
      + arn                         = (known after apply)
      + bucket                      = "reimei-b2b-demo-gpu-prd-tfstate"
      + bucket_domain_name          = (known after apply)
      + bucket_prefix               = (known after apply)
      + bucket_region               = (known after apply)
      + bucket_regional_domain_name = (known after apply)
      + force_destroy               = false
      + hosted_zone_id              = (known after apply)
      + id                          = (known after apply)
      + object_lock_enabled         = (known after apply)
      + policy                      = (known after apply)
      + region                      = "ap-northeast-1"
      + request_payer               = (known after apply)
      + tags_all                    = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + website_domain              = (known after apply)
      + website_endpoint            = (known after apply)

      + cors_rule (known after apply)

      + grant (known after apply)

      + lifecycle_rule (known after apply)

      + logging (known after apply)

      + object_lock_configuration (known after apply)

      + replication_configuration (known after apply)

      + server_side_encryption_configuration (known after apply)

      + versioning (known after apply)

      + website (known after apply)
    }

  # module.s3_tfstate.aws_s3_bucket_versioning.versioning_tfstate_bucket will be created
  + resource "aws_s3_bucket_versioning" "versioning_tfstate_bucket" {
      + bucket = (known after apply)
      + id     = (known after apply)
      + region = "ap-northeast-1"

      + versioning_configuration {
          + mfa_delete = (known after apply)
          + status     = "Enabled"
        }
    }

  # module.s3_tfstate.local_file.backend will be created
  + resource "local_file" "backend" {
      + content              = (known after apply)
      + content_base64sha256 = (known after apply)
      + content_base64sha512 = (known after apply)
      + content_md5          = (known after apply)
      + content_sha1         = (known after apply)
      + content_sha256       = (known after apply)
      + content_sha512       = (known after apply)
      + directory_permission = "0777"
      + file_permission      = "0644"
      + filename             = "./backend.tf"
      + id                   = (known after apply)
    }

Plan: 17 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + bucket_name  = "reimei-b2b-demo-gpu-prd-model-storage"
  + nlb_dns_name = (known after apply)

─────────────────────────────────────────────────────────────────────────────

Note: You didn't use the -out option to save this plan, so Terraform can't
guarantee to take exactly these actions if you run "terraform apply" now.

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((b2b-demo-gpu-prod-release-20260121-1))
$ terraform apply
data.aws_vpc.reimei_rag_main_vpc: Reading...
data.aws_ecr_repository.gpu: Reading...
module.s3_tfstate.data.aws_caller_identity.current: Reading...
data.aws_subnet.reimei_rag_vpc_subnet_private01: Reading...
data.aws_subnet.reimei_rag_vpc_subnet_private03: Reading...
data.aws_subnet.reimei_rag_vpc_subnet_private02: Reading...
module.s3_tfstate.data.aws_caller_identity.current: Read complete after 0s [id=050451382758]
data.aws_subnet.reimei_rag_vpc_subnet_private02: Read complete after 0s [id=subnet-07c0c0990467fe98d]
data.aws_subnet.reimei_rag_vpc_subnet_private01: Read complete after 0s [id=subnet-0fcce540a819b1bb3]
data.aws_subnet.reimei_rag_vpc_subnet_private03: Read complete after 0s [id=subnet-0f36d0c08cda116c4]
data.aws_ecr_repository.gpu: Read complete after 0s [id=sbint-services/reimei-engine-ray-stg/llm-serve-gpu]
data.aws_vpc.reimei_rag_main_vpc: Read complete after 0s [id=vpc-07758e1658c4c2149]

Terraform used the selected providers to generate the following execution plan. Resource actions
are indicated with the following symbols:
  + create
 <= read (data resources)

Terraform will perform the following actions:

  # data.cloudinit_config.init will be read during apply
  # (depends on a resource or a module with changes pending)
 <= data "cloudinit_config" "init" {
      + base64_encode = false
      + boundary      = (known after apply)
      + gzip          = false
      + id            = (known after apply)
      + rendered      = (known after apply)

      + part {
          + content      = <<-EOT
                #cloud-config
                cloud_final_modules:
                - [ scripts-user, always ]
            EOT
          + content_type = "text/cloud-config"
        }
      + part {
          + content      = <<-EOT
                #!/bin/bash
                set -eux -o pipefail

                # Create config.yaml
                cat > /home/ec2-user/config.yaml << YAML
                http_options:
                  host: 0.0.0.0
                  port: 8000
                  keep_alive_timeout_s: 200

                # https://docs.ray.io/en/latest/serve/monitoring.html#configure-serve-logging
                logging_config:
                  log_level: DEBUG
                  logs_dir: null # defaults to ray temp dir
                  encoding: JSON
                  enable_access_log: true

                applications:
                  - name: llm-inference-app
                    route_prefix: /
                    import_path: llm_serve.app.builder:build_openai_app
                    runtime_env:
                      env_vars:
                        RAY_LOG_TO_DRIVER: '1'
                        RAY_LOG_TO_DRIVER_EVENT_LEVEL: INFO
                        VLLM_NO_USAGE_STATS: '1'
                    args:
                      deployment_options:
                        max_ongoing_requests: 64
                        name: llm-server-router
                        num_replicas: 8
                        ray_actor_options:
                          num_cpus: 0.1
                      llm_server_configs:
                        - deployment_options:
                            max_ongoing_requests: 128
                            name: sarashinachat-7b
                            num_replicas: 1
                            ray_actor_options:
                              num_cpus: 0.1
                              num_gpus: 0.4
                          engine: vllm
                          engine_kwargs:
                            model: /config/models/7b-2512-sft-dpo
                            override_generation_config:
                              repetition_penalty: 1.05
                              temperature: 0.7
                              top_p: 0.9
                            served_model_name:
                              - sbint-instruct-2025-11-25:70b-sft
                              - sarashina2-mini
                              - sarashina-mini
                              - tmp-new-70b-sft-safety
                            gpu_memory_utilization: 0.4
                            max_model_len: 16384
                            num_gpu_blocks_override: 1024
                            enforce_eager: true
                          server_kwargs:
                            enable_auto_tool_choice: true
                            tool_call_parser: sarashina2_json
                        - deployment_options:
                            max_ongoing_requests: 128
                            name: sarashinaguard-7b
                            num_replicas: 1
                            ray_actor_options:
                              num_cpus: 0.1
                              num_gpus: 0.4
                          engine: vllm
                          engine_kwargs:
                            model: /config/models/sarashinaguard-7b-20250508
                            served_model_name:
                              - sarashinaguard-7b-20250508
                              - sarashinaguard-v2.0.1
                            gpu_memory_utilization: 0.4
                            max_model_len: 16384
                            num_gpu_blocks_override: 1024
                            enforce_eager: true
                        - deployment_options:
                            max_ongoing_requests: 128
                            name: sarashinaembedding-1b
                            num_replicas: 1
                            ray_actor_options:
                              num_cpus: 0.1
                              num_gpus: 0.1
                          engine: vllm
                          engine_kwargs:
                            model: /config/models/sarashina-embedding-v2-1b
                            served_model_name:
                              - sarashina-embedding-v2-1b
                            gpu_memory_utilization: 0.1
                            max_model_len: 8192
                            num_gpu_blocks_override: 512
                            enforce_eager: true
                        - deployment_options:
                            max_ongoing_requests: 128
                            name: sarashinareranker-0.5b
                            num_replicas: 1
                            ray_actor_options:
                              num_cpus: 0.1
                              num_gpus: 0.1
                          engine: vllm
                          engine_kwargs:
                            model: /config/models/sarashina2.2-0.5b-reranker-toy-0523
                            served_model_name:
                              - sarashina2.2-0.5b-reranker-toy-0523
                            gpu_memory_utilization: 0.1
                            max_model_len: 8192
                            num_gpu_blocks_override: 512
                            enforce_eager: true

                YAML

                aws s3 cp --recursive s3://reimei-b2b-demo-gpu-prd-model-storage /home/ec2-user/models
                ls -la /home/ec2-user/models
                aws ecr get-login-password --region ap-northeast-1 | docker login --username AWS --password-stdin 050451382758.dkr.ecr.ap-northeast-1.amazonaws.com/sbint-services/reimei-engine-ray-stg/llm-serve-gpu
                docker run --gpus all --rm --entrypoint '' -v /home/ec2-user:/config --network host -e MAX_PER_REPLICA_RETRY_COUNT=100 -e MAX_DEPLOYMENT_CONSTRUCTOR_RETRY_COUNT=100 -e TZ=UTC \
                  050451382758.dkr.ecr.ap-northeast-1.amazonaws.com/sbint-services/reimei-engine-ray-stg/llm-serve-gpu:20251224-1 serve run /config/config.yaml
            EOT
          + content_type = "text/x-shellscript"
          + filename     = "setup_ec2.sh"
        }
    }

  # aws_lb.nlb will be created
  + resource "aws_lb" "nlb" {
      + arn                                                          = (known after apply)
      + arn_suffix                                                   = (known after apply)
      + dns_name                                                     = (known after apply)
      + dns_record_client_routing_policy                             = "any_availability_zone"
      + enable_cross_zone_load_balancing                             = false
      + enable_deletion_protection                                   = false
      + enable_zonal_shift                                           = false
      + enforce_security_group_inbound_rules_on_private_link_traffic = (known after apply)
      + id                                                           = (known after apply)
      + internal                                                     = true
      + ip_address_type                                              = (known after apply)
      + load_balancer_type                                           = "network"
      + name                                                         = "reimei-b2b-demo-gpu-prd-nlb"
      + name_prefix                                                  = (known after apply)
      + region                                                       = "ap-northeast-1"
      + secondary_ips_auto_assigned_per_subnet                       = (known after apply)
      + security_groups                                              = (known after apply)
      + subnets                                                      = [
          + "subnet-07c0c0990467fe98d",
          + "subnet-0f36d0c08cda116c4",
          + "subnet-0fcce540a819b1bb3",
        ]
      + tags_all                                                     = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + vpc_id                                                       = (known after apply)
      + zone_id                                                      = (known after apply)

      + subnet_mapping (known after apply)
    }

  # aws_lb_listener.nlb will be created
  + resource "aws_lb_listener" "nlb" {
      + arn                                                                   = (known after apply)
      + id                                                                    = (known after apply)
      + load_balancer_arn                                                     = (known after apply)
      + port                                                                  = 8000
      + protocol                                                              = "TCP"
      + region                                                                = "ap-northeast-1"
      + routing_http_request_x_amzn_mtls_clientcert_header_name               = (known after apply)
      + routing_http_request_x_amzn_mtls_clientcert_issuer_header_name        = (known after apply)
      + routing_http_request_x_amzn_mtls_clientcert_leaf_header_name          = (known after apply)
      + routing_http_request_x_amzn_mtls_clientcert_serial_number_header_name = (known after apply)
      + routing_http_request_x_amzn_mtls_clientcert_subject_header_name       = (known after apply)
      + routing_http_request_x_amzn_mtls_clientcert_validity_header_name      = (known after apply)
      + routing_http_request_x_amzn_tls_cipher_suite_header_name              = (known after apply)
      + routing_http_request_x_amzn_tls_version_header_name                   = (known after apply)
      + routing_http_response_access_control_allow_credentials_header_value   = (known after apply)
      + routing_http_response_access_control_allow_headers_header_value       = (known after apply)
      + routing_http_response_access_control_allow_methods_header_value       = (known after apply)
      + routing_http_response_access_control_allow_origin_header_value        = (known after apply)
      + routing_http_response_access_control_expose_headers_header_value      = (known after apply)
      + routing_http_response_access_control_max_age_header_value             = (known after apply)
      + routing_http_response_content_security_policy_header_value            = (known after apply)
      + routing_http_response_server_enabled                                  = (known after apply)
      + routing_http_response_strict_transport_security_header_value          = (known after apply)
      + routing_http_response_x_content_type_options_header_value             = (known after apply)
      + routing_http_response_x_frame_options_header_value                    = (known after apply)
      + ssl_policy                                                            = (known after apply)
      + tags_all                                                              = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + tcp_idle_timeout_seconds                                              = (known after apply)

      + default_action {
          + order            = (known after apply)
          + target_group_arn = (known after apply)
          + type             = "forward"
        }

      + mutual_authentication (known after apply)
    }

  # aws_lb_target_group.nlb will be created
  + resource "aws_lb_target_group" "nlb" {
      + arn                                = (known after apply)
      + arn_suffix                         = (known after apply)
      + connection_termination             = (known after apply)
      + deregistration_delay               = "300"
      + id                                 = (known after apply)
      + ip_address_type                    = (known after apply)
      + lambda_multi_value_headers_enabled = false
      + load_balancer_arns                 = (known after apply)
      + load_balancing_algorithm_type      = (known after apply)
      + load_balancing_anomaly_mitigation  = (known after apply)
      + load_balancing_cross_zone_enabled  = (known after apply)
      + name                               = "reimei-b2b-demo-gpu-prd-tg"
      + name_prefix                        = (known after apply)
      + port                               = 8000
      + preserve_client_ip                 = (known after apply)
      + protocol                           = "TCP"
      + protocol_version                   = (known after apply)
      + proxy_protocol_v2                  = false
      + region                             = "ap-northeast-1"
      + slow_start                         = 0
      + tags_all                           = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + target_type                        = "instance"
      + vpc_id                             = "vpc-07758e1658c4c2149"

      + health_check {
          + enabled             = true
          + healthy_threshold   = 2
          + interval            = 30
          + matcher             = "200"
          + path                = "/health"
          + port                = "traffic-port"
          + protocol            = "HTTP"
          + timeout             = 5
          + unhealthy_threshold = 3
        }

      + stickiness (known after apply)

      + target_failover (known after apply)

      + target_group_health (known after apply)

      + target_health_state (known after apply)
    }

  # aws_lb_target_group_attachment.nlb will be created
  + resource "aws_lb_target_group_attachment" "nlb" {
      + id               = (known after apply)
      + port             = 8000
      + region           = "ap-northeast-1"
      + target_group_arn = (known after apply)
      + target_id        = (known after apply)
    }

  # aws_s3_bucket.model_storage will be created
  + resource "aws_s3_bucket" "model_storage" {
      + acceleration_status         = (known after apply)
      + acl                         = (known after apply)
      + arn                         = (known after apply)
      + bucket                      = "reimei-b2b-demo-gpu-prd-model-storage"
      + bucket_domain_name          = (known after apply)
      + bucket_prefix               = (known after apply)
      + bucket_region               = (known after apply)
      + bucket_regional_domain_name = (known after apply)
      + force_destroy               = false
      + hosted_zone_id              = (known after apply)
      + id                          = (known after apply)
      + object_lock_enabled         = (known after apply)
      + policy                      = (known after apply)
      + region                      = "ap-northeast-1"
      + request_payer               = (known after apply)
      + tags_all                    = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + website_domain              = (known after apply)
      + website_endpoint            = (known after apply)

      + cors_rule (known after apply)

      + grant (known after apply)

      + lifecycle_rule (known after apply)

      + logging (known after apply)

      + object_lock_configuration (known after apply)

      + replication_configuration (known after apply)

      + server_side_encryption_configuration (known after apply)

      + versioning (known after apply)

      + website (known after apply)
    }

  # aws_security_group.gpu_instance will be created
  + resource "aws_security_group" "gpu_instance" {
      + arn                    = (known after apply)
      + description            = "Security group for demo inference GPU instances"
      + egress                 = [
          + {
              + cidr_blocks      = [
                  + "0.0.0.0/0",
                ]
              + from_port        = 0
              + ipv6_cidr_blocks = []
              + prefix_list_ids  = []
              + protocol         = "-1"
              + security_groups  = []
              + self             = false
              + to_port          = 0
                # (1 unchanged attribute hidden)
            },
        ]
      + id                     = (known after apply)
      + ingress                = [
          + {
              + cidr_blocks      = [
                  + "10.31.140.0/22",
                  + "10.31.144.0/22",
                  + "10.31.148.0/22",
                ]
              + from_port        = 8000
              + ipv6_cidr_blocks = []
              + prefix_list_ids  = []
              + protocol         = "tcp"
              + security_groups  = []
              + self             = false
              + to_port          = 8000
                # (1 unchanged attribute hidden)
            },
        ]
      + name                   = "reimei-b2b-demo-gpu-prd-gpu-instance-sg"
      + name_prefix            = (known after apply)
      + owner_id               = (known after apply)
      + region                 = "ap-northeast-1"
      + revoke_rules_on_delete = false
      + tags                   = {
          + "Name" = "reimei-b2b-demo-gpu-prd-gpu-instance-sg"
        }
      + tags_all               = {
          + "Env"         = "prd"
          + "Name"        = "reimei-b2b-demo-gpu-prd-gpu-instance-sg"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + vpc_id                 = "vpc-07758e1658c4c2149"
    }

  # module.gpu_instance.aws_cloudwatch_log_group.ec2_logs will be created
  + resource "aws_cloudwatch_log_group" "ec2_logs" {
      + arn               = (known after apply)
      + id                = (known after apply)
      + log_group_class   = (known after apply)
      + name              = "/ec2/reimei-b2b-demo-gpu-prd/gpu-inference"
      + name_prefix       = (known after apply)
      + region            = "ap-northeast-1"
      + retention_in_days = 30
      + skip_destroy      = false
      + tags_all          = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
    }

  # module.gpu_instance.aws_iam_instance_profile.ec2_profile will be created
  + resource "aws_iam_instance_profile" "ec2_profile" {
      + arn         = (known after apply)
      + create_date = (known after apply)
      + id          = (known after apply)
      + name        = "reimei-b2b-demo-gpu-prd-gpu-inference-instance-profile"
      + name_prefix = (known after apply)
      + path        = "/"
      + role        = "reimei-b2b-demo-gpu-prd-gpu-inference-role"
      + tags_all    = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + unique_id   = (known after apply)
    }

  # module.gpu_instance.aws_iam_role.ec2_role will be created
  + resource "aws_iam_role" "ec2_role" {
      + arn                   = (known after apply)
      + assume_role_policy    = jsonencode(
            {
              + Statement = [
                  + {
                      + Action    = "sts:AssumeRole"
                      + Effect    = "Allow"
                      + Principal = {
                          + Service = "ec2.amazonaws.com"
                        }
                    },
                ]
              + Version   = "2012-10-17"
            }
        )
      + create_date           = (known after apply)
      + force_detach_policies = false
      + id                    = (known after apply)
      + managed_policy_arns   = (known after apply)
      + max_session_duration  = 3600
      + name                  = "reimei-b2b-demo-gpu-prd-gpu-inference-role"
      + name_prefix           = (known after apply)
      + path                  = "/"
      + tags_all              = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + unique_id             = (known after apply)

      + inline_policy (known after apply)
    }

  # module.gpu_instance.aws_iam_role_policy_attachment.custom_policies["arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly"] will be created
  + resource "aws_iam_role_policy_attachment" "custom_policies" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly"
      + role       = "reimei-b2b-demo-gpu-prd-gpu-inference-role"
    }

  # module.gpu_instance.aws_iam_role_policy_attachment.custom_policies["arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"] will be created
  + resource "aws_iam_role_policy_attachment" "custom_policies" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
      + role       = "reimei-b2b-demo-gpu-prd-gpu-inference-role"
    }

  # module.gpu_instance.aws_iam_role_policy_attachment.cw_agent_managed will be created
  + resource "aws_iam_role_policy_attachment" "cw_agent_managed" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy"
      + role       = "reimei-b2b-demo-gpu-prd-gpu-inference-role"
    }

  # module.gpu_instance.aws_iam_role_policy_attachment.ssm_managed will be created
  + resource "aws_iam_role_policy_attachment" "ssm_managed" {
      + id         = (known after apply)
      + policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
      + role       = "reimei-b2b-demo-gpu-prd-gpu-inference-role"
    }

  # module.gpu_instance.aws_instance.this will be created
  + resource "aws_instance" "this" {
      + ami                                  = "ami-0591d1602f530f886"
      + arn                                  = (known after apply)
      + associate_public_ip_address          = (known after apply)
      + availability_zone                    = (known after apply)
      + disable_api_stop                     = (known after apply)
      + disable_api_termination              = (known after apply)
      + ebs_optimized                        = (known after apply)
      + enable_primary_ipv6                  = (known after apply)
      + force_destroy                        = false
      + get_password_data                    = false
      + host_id                              = (known after apply)
      + host_resource_group_arn              = (known after apply)
      + iam_instance_profile                 = "reimei-b2b-demo-gpu-prd-gpu-inference-instance-profile"
      + id                                   = (known after apply)
      + instance_initiated_shutdown_behavior = (known after apply)
      + instance_lifecycle                   = (known after apply)
      + instance_state                       = (known after apply)
      + instance_type                        = "g6e.xlarge"
      + ipv6_address_count                   = (known after apply)
      + ipv6_addresses                       = (known after apply)
      + key_name                             = (known after apply)
      + monitoring                           = (known after apply)
      + outpost_arn                          = (known after apply)
      + password_data                        = (known after apply)
      + placement_group                      = (known after apply)
      + placement_group_id                   = (known after apply)
      + placement_partition_number           = (known after apply)
      + primary_network_interface_id         = (known after apply)
      + private_dns                          = (known after apply)
      + private_ip                           = "10.31.140.50"
      + public_dns                           = (known after apply)
      + public_ip                            = (known after apply)
      + region                               = "ap-northeast-1"
      + secondary_private_ips                = (known after apply)
      + security_groups                      = (known after apply)
      + source_dest_check                    = true
      + spot_instance_request_id             = (known after apply)
      + subnet_id                            = "subnet-0fcce540a819b1bb3"
      + tags                                 = {
          + "Name" = "reimei-b2b-demo-gpu-prd-gpu-inference"
        }
      + tags_all                             = {
          + "Env"         = "prd"
          + "Name"        = "reimei-b2b-demo-gpu-prd-gpu-inference"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "component"   = "gpu-inference"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + tenancy                              = (known after apply)
      + user_data                            = (known after apply)
      + user_data_base64                     = (known after apply)
      + user_data_replace_on_change          = false
      + vpc_security_group_ids               = (known after apply)

      + capacity_reservation_specification (known after apply)

      + cpu_options (known after apply)

      + ebs_block_device (known after apply)

      + enclave_options (known after apply)

      + ephemeral_block_device (known after apply)

      + instance_market_options (known after apply)

      + maintenance_options (known after apply)

      + metadata_options (known after apply)

      + network_interface (known after apply)

      + primary_network_interface (known after apply)

      + private_dns_name_options (known after apply)

      + root_block_device {
          + delete_on_termination = true
          + device_name           = (known after apply)
          + encrypted             = (known after apply)
          + iops                  = 3000
          + kms_key_id            = (known after apply)
          + tags_all              = (known after apply)
          + throughput            = 125
          + volume_id             = (known after apply)
          + volume_size           = 120
          + volume_type           = "gp3"
        }
    }

  # module.s3_tfstate.aws_s3_bucket.tfstate_bucket will be created
  + resource "aws_s3_bucket" "tfstate_bucket" {
      + acceleration_status         = (known after apply)
      + acl                         = (known after apply)
      + arn                         = (known after apply)
      + bucket                      = "reimei-b2b-demo-gpu-prd-tfstate"
      + bucket_domain_name          = (known after apply)
      + bucket_prefix               = (known after apply)
      + bucket_region               = (known after apply)
      + bucket_regional_domain_name = (known after apply)
      + force_destroy               = false
      + hosted_zone_id              = (known after apply)
      + id                          = (known after apply)
      + object_lock_enabled         = (known after apply)
      + policy                      = (known after apply)
      + region                      = "ap-northeast-1"
      + request_payer               = (known after apply)
      + tags_all                    = {
          + "Env"         = "prd"
          + "Project"     = "reimei-b2b-demo"
          + "Terraform"   = "true"
          + "environment" = "prod"
          + "product"     = "reimei-b2b-demo"
        }
      + website_domain              = (known after apply)
      + website_endpoint            = (known after apply)

      + cors_rule (known after apply)

      + grant (known after apply)

      + lifecycle_rule (known after apply)

      + logging (known after apply)

      + object_lock_configuration (known after apply)

      + replication_configuration (known after apply)

      + server_side_encryption_configuration (known after apply)

      + versioning (known after apply)

      + website (known after apply)
    }

  # module.s3_tfstate.aws_s3_bucket_versioning.versioning_tfstate_bucket will be created
  + resource "aws_s3_bucket_versioning" "versioning_tfstate_bucket" {
      + bucket = (known after apply)
      + id     = (known after apply)
      + region = "ap-northeast-1"

      + versioning_configuration {
          + mfa_delete = (known after apply)
          + status     = "Enabled"
        }
    }

  # module.s3_tfstate.local_file.backend will be created
  + resource "local_file" "backend" {
      + content              = (known after apply)
      + content_base64sha256 = (known after apply)
      + content_base64sha512 = (known after apply)
      + content_md5          = (known after apply)
      + content_sha1         = (known after apply)
      + content_sha256       = (known after apply)
      + content_sha512       = (known after apply)
      + directory_permission = "0777"
      + file_permission      = "0644"
      + filename             = "./backend.tf"
      + id                   = (known after apply)
    }

Plan: 17 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + bucket_name  = "reimei-b2b-demo-gpu-prd-model-storage"
  + nlb_dns_name = (known after apply)

Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

  Enter a value: yes

module.gpu_instance.aws_cloudwatch_log_group.ec2_logs: Creating...
module.gpu_instance.aws_iam_role.ec2_role: Creating...
aws_s3_bucket.model_storage: Creating...
aws_lb.nlb: Creating...
aws_security_group.gpu_instance: Creating...
aws_lb_target_group.nlb: Creating...
module.s3_tfstate.aws_s3_bucket.tfstate_bucket: Creating...
module.gpu_instance.aws_cloudwatch_log_group.ec2_logs: Creation complete after 0s [id=/ec2/reimei-b2b-demo-gpu-prd/gpu-inference]
aws_lb_target_group.nlb: Creation complete after 1s [id=arn:aws:elasticloadbalancing:ap-northeast-1:050451382758:targetgroup/reimei-b2b-demo-gpu-prd-tg/6e2df432a4add948]
aws_s3_bucket.model_storage: Creation complete after 1s [id=reimei-b2b-demo-gpu-prd-model-storage]
data.cloudinit_config.init: Reading...
data.cloudinit_config.init: Read complete after 0s [id=1108901483]
module.s3_tfstate.aws_s3_bucket.tfstate_bucket: Creation complete after 1s [id=reimei-b2b-demo-gpu-prd-tfstate]
module.s3_tfstate.aws_s3_bucket_versioning.versioning_tfstate_bucket: Creating...
module.s3_tfstate.local_file.backend: Creating...
module.s3_tfstate.local_file.backend: Creation complete after 0s [id=693e750e973ef8158dc4ef584341188cc4c3f2d6]
module.gpu_instance.aws_iam_role.ec2_role: Creation complete after 1s [id=reimei-b2b-demo-gpu-prd-gpu-inference-role]
module.gpu_instance.aws_iam_role_policy_attachment.cw_agent_managed: Creating...
module.gpu_instance.aws_iam_role_policy_attachment.custom_policies["arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly"]: Creating...
module.gpu_instance.aws_iam_role_policy_attachment.ssm_managed: Creating...
module.gpu_instance.aws_iam_role_policy_attachment.custom_policies["arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"]: Creating...
module.gpu_instance.aws_iam_instance_profile.ec2_profile: Creating...
aws_security_group.gpu_instance: Creation complete after 2s [id=sg-020d5c933c9a4f4ac]
module.gpu_instance.aws_iam_role_policy_attachment.custom_policies["arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly"]: Creation complete after 1s [id=reimei-b2b-demo-gpu-prd-gpu-inference-role/arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPullOnly]
module.gpu_instance.aws_iam_role_policy_attachment.cw_agent_managed: Creation complete after 1s [id=reimei-b2b-demo-gpu-prd-gpu-inference-role/arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy]
module.gpu_instance.aws_iam_role_policy_attachment.ssm_managed: Creation complete after 1s [id=reimei-b2b-demo-gpu-prd-gpu-inference-role/arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore]
module.gpu_instance.aws_iam_role_policy_attachment.custom_policies["arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"]: Creation complete after 1s [id=reimei-b2b-demo-gpu-prd-gpu-inference-role/arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess]
module.s3_tfstate.aws_s3_bucket_versioning.versioning_tfstate_bucket: Creation complete after 2s [id=reimei-b2b-demo-gpu-prd-tfstate]
module.gpu_instance.aws_iam_instance_profile.ec2_profile: Creation complete after 8s [id=reimei-b2b-demo-gpu-prd-gpu-inference-instance-profile]
module.gpu_instance.aws_instance.this: Creating...
aws_lb.nlb: Still creating... [00m10s elapsed]
module.gpu_instance.aws_instance.this: Still creating... [00m10s elapsed]
aws_lb.nlb: Still creating... [00m20s elapsed]
module.gpu_instance.aws_instance.this: Creation complete after 15s [id=i-041be5eed07d17f8f]
aws_lb_target_group_attachment.nlb: Creating...
aws_lb_target_group_attachment.nlb: Creation complete after 0s [id=arn:aws:elasticloadbalancing:ap-northeast-1:050451382758:targetgroup/reimei-b2b-demo-gpu-prd-tg/6e2df432a4add948-20260121041818969400000002]
aws_lb.nlb: Still creating... [00m30s elapsed]
aws_lb.nlb: Still creating... [00m40s elapsed]
aws_lb.nlb: Still creating... [00m50s elapsed]
aws_lb.nlb: Still creating... [01m00s elapsed]
aws_lb.nlb: Still creating... [01m10s elapsed]
aws_lb.nlb: Still creating... [01m20s elapsed]
aws_lb.nlb: Still creating... [01m30s elapsed]
aws_lb.nlb: Still creating... [01m40s elapsed]
aws_lb.nlb: Still creating... [01m50s elapsed]
aws_lb.nlb: Still creating... [02m00s elapsed]
aws_lb.nlb: Still creating... [02m10s elapsed]
aws_lb.nlb: Still creating... [02m20s elapsed]
aws_lb.nlb: Still creating... [02m30s elapsed]
aws_lb.nlb: Still creating... [02m40s elapsed]
aws_lb.nlb: Creation complete after 2m42s [id=arn:aws:elasticloadbalancing:ap-northeast-1:050451382758:loadbalancer/net/reimei-b2b-demo-gpu-prd-nlb/070877994bd3af97]
aws_lb_listener.nlb: Creating...
aws_lb_listener.nlb: Creation complete after 0s [id=arn:aws:elasticloadbalancing:ap-northeast-1:050451382758:listener/net/reimei-b2b-demo-gpu-prd-nlb/070877994bd3af97/e138aa077ab18099]

Apply complete! Resources: 17 added, 0 changed, 0 destroyed.

Outputs:

bucket_name = "reimei-b2b-demo-gpu-prd-model-storage"
nlb_dns_name = "reimei-b2b-demo-gpu-prd-nlb-070877994bd3af97.elb.ap-northeast-1.amazonaws.com"

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((b2b-demo-gpu-prod-release-20260121-1))
$ terraform init -migrate-state
Initializing the backend...
Do you want to copy existing state to the new backend?
  Pre-existing state was found while migrating the previous "local" backend to the
  newly configured "s3" backend. No existing state was found in the newly
  configured "s3" backend. Do you want to copy this state to the new "s3"
  backend? Enter "yes" to copy and "no" to start with an empty state.

  Enter a value: yes


Successfully configured the backend "s3"! Terraform will automatically
use this backend unless the backend configuration changes.
Initializing modules...
Initializing provider plugins...
- Reusing previous version of hashicorp/aws from the dependency lock file
- Reusing previous version of hashicorp/cloudinit from the dependency lock file
- Reusing previous version of hashicorp/local from the dependency lock file
- Using previously-installed hashicorp/cloudinit v2.3.7
- Using previously-installed hashicorp/local v2.6.1
- Using previously-installed hashicorp/aws v6.24.0

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((b2b-demo-gpu-prod-release-20260121-1))
$ git add backend.tf

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((b2b-demo-gpu-prod-release-20260121-1))
$ git commit -m "add backend.tf"
[detached HEAD 00a7967d] add backend.tf
 Committer: Shogo Kagami <prod.shogo.kagami@L4VSOC.SBINT.LOCAL>
Your name and email address were configured automatically based
on your username and hostname. Please check that they are accurate.
You can suppress this message by setting them explicitly:

    git config --global user.name "Your Name"
    git config --global user.email you@example.com

After doing this, you may fix the identity used for this commit with:

    git commit --amend --reset-author

 1 file changed, 9 insertions(+)
 create mode 100644 terraform/environments_b2b_demo_gpu/prod/backend.tf

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git push
fatal: You are not currently on a branch.
To push the history leading to the current (detached HEAD)
state now, use

    git push origin HEAD:<name-of-remote-branch>


prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git log
commit 00a7967d2eb47a58c7ed889c0a859138ec7b23c4 (HEAD)
Author: Shogo Kagami <prod.shogo.kagami@L4VSOC.SBINT.LOCAL>
Date:   Wed Jan 21 13:26:48 2026 +0900

    add backend.tf

commit c337cd2210d43520e0fbdd8335c51a19631744bb (tag: b2b-demo-gpu-prod-release-20260121-1)
Merge: d24c4b84 7e01507b
Author: Takeshi Oura <takoika@users.noreply.github.com>
Date:   Tue Jan 20 15:44:08 2026 +0900

    Merge pull request #881 from sbintuitions/toura-release-news

    共同通信ニュースとソフトバンクニュースをsbkk demo環境でリリースする

commit d24c4b84de6d0c9cc6106f14071b699d4a7cfb0e
Author: tatsuya-hayashino <tatsuya.hayashino@sbintuitions.co.jp>
Date:   Tue Jan 20 14:51:05 2026 +0900

    Add/Update API keys (#879)

commit cac7129a6f6d7259871facf52aeed91cc912b957
Merge: 931929ca 103c3aab
Author: Takeshi Oura <takoika@users.noreply.github.com>
Date:   Tue Jan 20 13:43:00 2026 +0900

    Merge pull request #880 from sbintuitions/toura-remove-old-vars

    デプロイ時のエラーを避けるために後方互換のために残していたenv varの削除

commit 103c3aab20b8a9337ccd13c3fef1af483ca59cab
Author: takoika <16839037+takoika@users.noreply.github.com>
Date:   Tue Jan 20 13:32:09 2026 +0900

    Remove unnecessary legacy env var

commit 86a0a6a83da00f7bf9200fb562554bbf398e8192
Merge: 27ade62d 931929ca
Author: takoika <16839037+takoika@users.noreply.github.com>

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ cd /c/Users/prod.shogo.kagami/Desktop/Prdb2b_demochat

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/Desktop/Prdb2b_demochat (main)
$ ls
7b-2512-sft-dpo/            sarashina2.2-0.5b-reranker-toy-0523/  sbint-translation-2025-12-22-dpo/
sarashina-embedding-v2-1b/  sarashinaguard-7b-20250508/

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/Desktop/Prdb2b_demochat (main)
$ ls-l
bash: ls-l: command not found

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/Desktop/Prdb2b_demochat (main)
$ ls -l
total 20
drwxr-xr-x 1 prod.shogo.kagami 1049089 0 Jan 21 11:43 7b-2512-sft-dpo/
drwxr-xr-x 1 prod.shogo.kagami 1049089 0 Jan 21 11:47 sarashina-embedding-v2-1b/
drwxr-xr-x 1 prod.shogo.kagami 1049089 0 Jan 21 11:44 sarashina2.2-0.5b-reranker-toy-0523/
drwxr-xr-x 1 prod.shogo.kagami 1049089 0 Jan 21 11:49 sarashinaguard-7b-20250508/
drwxr-xr-x 1 prod.shogo.kagami 1049089 0 Jan 21 11:54 sbint-translation-2025-12-22-dpo/

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/Desktop/Prdb2b_demochat (main)
$ AWS_PROFILE=AWSAdministratorAccess-050451382758 aws s3 cp --recursive . s3://reimei-b2b-demo-gpu-prd-model-storage
upload: 7b-2512-sft-dpo\mergekit_config.yml to s3://reimei-b2b-demo-gpu-prd-model-storage/7b-2512-sft-dpo/mergekit_config.yml
upload: 7b-2512-sft-dpo\special_tokens_map.json to s3://reimei-b2b-demo-gpu-prd-model-storage/7b-2512-sft-dpo/special_tokens_map.json
upload: 7b-2512-sft-dpo\tokenizer.model to s3://reimei-b2b-demo-gpu-prd-model-storage/7b-2512-sft-dpo/tokenizer.model
upload: 7b-2512-sft-dpo\config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/7b-2512-sft-dpo/config.json
upload: sarashina-embedding-v2-1b\1_Pooling\config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina-embedding-v2-1b/1_Pooling/config.json
upload: sarashina-embedding-v2-1b\config_sentence_transformers.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina-embedding-v2-1b/config_sentence_transformers.json
upload: 7b-2512-sft-dpo\tokenizer_config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/7b-2512-sft-dpo/tokenizer_config.json
upload: sarashina-embedding-v2-1b\README.md to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina-embedding-v2-1b/README.md
upload: 7b-2512-sft-dpo\model.safetensors.index.json to s3://reimei-b2b-demo-gpu-prd-model-storage/7b-2512-sft-dpo/model.safetensors.index.json
upload: sarashina-embedding-v2-1b\merge_config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina-embedding-v2-1b/merge_config.json
upload: sarashina-embedding-v2-1b\config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina-embedding-v2-1b/config.json
upload: sarashina-embedding-v2-1b\sentence_bert_config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina-embedding-v2-1b/sentence_bert_config.json
upload: sarashina-embedding-v2-1b\special_tokens_map.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina-embedding-v2-1b/special_tokens_map.json
upload: sarashina-embedding-v2-1b\tokenizer.model to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina-embedding-v2-1b/tokenizer.model
upload: sarashina-embedding-v2-1b\tokenizer_config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina-embedding-v2-1b/tokenizer_config.json
upload: sarashina2.2-0.5b-reranker-toy-0523\README.md to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina2.2-0.5b-reranker-toy-0523/README.md
upload: sarashina2.2-0.5b-reranker-toy-0523\config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina2.2-0.5b-reranker-toy-0523/config.json
upload: sarashina-embedding-v2-1b\tokenizer.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina-embedding-v2-1b/tokenizer.json
upload: sarashina2.2-0.5b-reranker-toy-0523\special_tokens_map.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina2.2-0.5b-reranker-toy-0523/special_tokens_map.json
upload: sarashina2.2-0.5b-reranker-toy-0523\tokenizer_config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina2.2-0.5b-reranker-toy-0523/tokenizer_config.json
upload: sarashinaguard-7b-20250508\README.md to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/README.md
upload: sarashinaguard-7b-20250508\config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/config.json
upload: sarashinaguard-7b-20250508\mergekit_config.yml to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/mergekit_config.yml
upload: sarashina2.2-0.5b-reranker-toy-0523\tokenizer.model to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina2.2-0.5b-reranker-toy-0523/tokenizer.model
upload: sarashina2.2-0.5b-reranker-toy-0523\tokenizer.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina2.2-0.5b-reranker-toy-0523/tokenizer.json
upload: sarashina2.2-0.5b-reranker-toy-0523\model.safetensors to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina2.2-0.5b-reranker-toy-0523/model.safetensors
upload: sarashina-embedding-v2-1b\modules.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina-embedding-v2-1b/modules.json
upload: sarashinaguard-7b-20250508\model-00001-of-00003.safetensors to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/model-00001-of-00003.safetensors
upload: sarashinaguard-7b-20250508\model.safetensors.index.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/model.safetensors.index.json
upload: sarashinaguard-7b-20250508\special_tokens_map.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/special_tokens_map.json
upload: 7b-2512-sft-dpo\model-00003-of-00003.safetensors to s3://reimei-b2b-demo-gpu-prd-model-storge/7b-2512-sft-dpo/model-00003-of-00003.safetensors
upload: sarashinaguard-7b-20250508\test.predictions.jsonl to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/test.predictions.jsonl
upload: sarashinaguard-7b-20250508\test.predictions.jsonl.csv to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/test.predictions.jsonl.csv
upload: sarashinaguard-7b-20250508\tokenizer.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/tokenizer.json
upload: sarashinaguard-7b-20250508\tokenizer.model to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/tokenizer.model
upload: sarashinaguard-7b-20250508\tokenizer_config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/tokenizer_config.json
upload: sbint-translation-2025-12-22-dpo\README.md to s3://reimei-b2b-demo-gpu-prd-model-storage/sbint-translation-2025-12-22-dpo/README.md
upload: sbint-translation-2025-12-22-dpo\config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sbint-translation-2025-12-22-dpo/config.json
upload: sbint-translation-2025-12-22-dpo\mergekit_config.yml to s3://reimei-b2b-demo-gpu-prd-model-storage/sbint-translation-2025-12-22-dpo/mergekit_config.yml
upload: sarashina-embedding-v2-1b\model.safetensors to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashina-embedding-v2-1b/model.safetensors
upload: sarashinaguard-7b-20250508\model-00002-of-00003.safetensors to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/model-00002-of-00003.safetensors
upload: sarashinaguard-7b-20250508\model-00003-of-00003.safetensors to s3://reimei-b2b-demo-gpu-prd-model-storage/sarashinaguard-7b-20250508/model-00003-of-00003.safetensors
upload: sbint-translation-2025-12-22-dpo\model.safetensors.index.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sbint-translation-2025-12-22-dpo/model.safetensors.index.json
upload: sbint-translation-2025-12-22-dpo\prompt_template.enja.jinja to s3://reimei-b2b-demo-gpu-prd-model-storage/sbint-translation-2025-12-22-dpo/prompt_template.enja.jinja
upload: sbint-translation-2025-12-22-dpo\prompt_template.jaen.jinja to s3://reimei-b2b-demo-gpu-prd-model-storage/sbint-translation-2025-12-22-dpo/prompt_template.jaen.jinja
upload: sbint-translation-2025-12-22-dpo\special_tokens_map.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sbint-translation-2025-12-22-dpo/special_tokens_map.json
upload: sbint-translation-2025-12-22-dpo\tokenizer.model to s3://reimei-b2b-demo-gpu-prd-model-storage/sbint-translation-2025-12-22-dpo/tokenizer.model
upload: sbint-translation-2025-12-22-dpo\tokenizer_config.json to s3://reimei-b2b-demo-gpu-prd-model-storage/sbint-translation-2025-12-22-dpo/tokenizer_config.json
upload: 7b-2512-sft-dpo\model-00002-of-00003.safetensors to s3://reimei-b2b-demo-gpu-prd-model-storage/7b-2512-sft-dpo/model-00002-of-00003.safetensors
upload: sbint-translation-2025-12-22-dpo\model-00003-of-00003.safetensors to s3://reimei-b2b-demo-gpu-prd-model-storage/sbint-translation-2025-12-22-dpo/model-00003-of-00003.safetensors
upload: sbint-translation-2025-12-22-dpo\model-00002-of-00003.safetensors to s3://reimei-b2b-demo-gpu-prd-model-storage/sbint-translation-2025-12-22-dpo/model-00002-of-00003.safetensors
upload: sbint-translation-2025-12-22-dpo\model-00001-of-00003.safetensors to s3://reimei-b2b-demo-gpu-prd-model-storage/sbint-translation-2025-12-22-dpo/model-00001-of-00003.safetensors

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/Desktop/Prdb2b_demochat (main)
$ cd ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git branch
* (no branch)

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ cd ..

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu ((00a7967d...))
$ git branch
* (no branch)

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu ((00a7967d...))
$ git log
commit 00a7967d2eb47a58c7ed889c0a859138ec7b23c4 (HEAD)
Author: Shogo Kagami <prod.shogo.kagami@L4VSOC.SBINT.LOCAL>
Date:   Wed Jan 21 13:26:48 2026 +0900

    add backend.tf

commit c337cd2210d43520e0fbdd8335c51a19631744bb (tag: b2b-demo-gpu-prod-release-20260121-1)
Merge: d24c4b84 7e01507b
Author: Takeshi Oura <takoika@users.noreply.github.com>
Date:   Tue Jan 20 15:44:08 2026 +0900

    Merge pull request #881 from sbintuitions/toura-release-news

    共同通信ニュースとソフトバンクニュースをsbkk demo環境でリリースする

commit d24c4b84de6d0c9cc6106f14071b699d4a7cfb0e
Author: tatsuya-hayashino <tatsuya.hayashino@sbintuitions.co.jp>
Date:   Tue Jan 20 14:51:05 2026 +0900

    Add/Update API keys (#879)

commit cac7129a6f6d7259871facf52aeed91cc912b957
Merge: 931929ca 103c3aab
Author: Takeshi Oura <takoika@users.noreply.github.com>
Date:   Tue Jan 20 13:43:00 2026 +0900

    Merge pull request #880 from sbintuitions/toura-remove-old-vars

    デプロイ時のエラーを避けるために後方互換のために残していたenv varの削除

commit 103c3aab20b8a9337ccd13c3fef1af483ca59cab
Author: takoika <16839037+takoika@users.noreply.github.com>
Date:   Tue Jan 20 13:32:09 2026 +0900

    Remove unnecessary legacy env var

commit 86a0a6a83da00f7bf9200fb562554bbf398e8192
Merge: 27ade62d 931929ca
Author: takoika <16839037+takoika@users.noreply.github.com>

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu ((00a7967d...))
$

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu ((00a7967d...))
$ git switch -b b2b-demo-gpu-prod-release-20260121-1
error: unknown switch `b'
usage: git switch [<options>] [<branch>]

    -c, --[no-]create <branch>
                          create and switch to a new branch
    -C, --[no-]force-create <branch>
                          create/reset and switch to a branch
    --[no-]guess          second guess 'git switch <no-such-branch>'
    --[no-]discard-changes
                          throw away local modifications
    -q, --[no-]quiet      suppress progress reporting
    --[no-]recurse-submodules[=<checkout>]
                          control recursive updating of submodules
    --[no-]progress       force progress reporting
    -m, --[no-]merge      perform a 3-way merge with the new branch
    --[no-]conflict <style>
                          conflict style (merge, diff3, or zdiff3)
    -d, --[no-]detach     detach HEAD at named commit
    -t, --[no-]track[=(direct|inherit)]
                          set branch tracking configuration
    -f, --[no-]force      force checkout (throw away local modifications)
    --[no-]orphan <new-branch>
                          new unborn branch
    --[no-]overwrite-ignore
                          update ignored files (default)
    --[no-]ignore-other-worktrees
                          do not check if another worktree is using this branch


prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu ((00a7967d...))
$ git fetch -b b2b-demo-gpu-prod-release-20260121-1
error: unknown switch `b'
usage: git fetch [<options>] [<repository> [<refspec>...]]
   or: git fetch [<options>] <group>
   or: git fetch --multiple [<options>] [(<repository> | <group>)...]
   or: git fetch --all [<options>]

    -v, --[no-]verbose    be more verbose
    -q, --[no-]quiet      be more quiet
    --[no-]all            fetch from all remotes
    --[no-]set-upstream   set upstream for git pull/fetch
    -a, --[no-]append     append to .git/FETCH_HEAD instead of overwriting
    --[no-]atomic         use atomic transaction to update references
    --[no-]upload-pack <path>
                          path to upload pack on remote end
    -f, --[no-]force      force overwrite of local reference
    -m, --[no-]multiple   fetch from multiple remotes
    -t, --[no-]tags       fetch all tags and associated objects
    -n                    do not fetch all tags (--no-tags)
    -j, --[no-]jobs <n>   number of submodules fetched in parallel
    --[no-]prefetch       modify the refspec to place all refs within refs/prefetch/
    -p, --[no-]prune      prune remote-tracking branches no longer on remote
    -P, --[no-]prune-tags prune local tags no longer on remote and clobber changed tags
    --[no-]recurse-submodules[=<on-demand>]
                          control recursive fetching of submodules
    --[no-]dry-run        dry run
    --[no-]porcelain      machine-readable output
    --[no-]write-fetch-head
                          write fetched references to the FETCH_HEAD file
    -k, --[no-]keep       keep downloaded pack
    -u, --[no-]update-head-ok
                          allow updating of HEAD ref
    --[no-]progress       force progress reporting
    --[no-]depth <depth>  deepen history of shallow clone
    --[no-]shallow-since <time>
                          deepen history of shallow repository based on time
    --[no-]shallow-exclude <ref>
                          deepen history of shallow clone, excluding ref
    --[no-]deepen <n>     deepen history of shallow clone
    --unshallow           convert to a complete repository
    --refetch             re-fetch without negotiating common commits
    --[no-]update-shallow accept refs that update .git/shallow
    --refmap <refmap>     specify fetch refmap
    -o, --[no-]server-option <server-specific>
                          option to transmit
    -4, --ipv4            use IPv4 addresses only
    -6, --ipv6            use IPv6 addresses only
    --[no-]negotiation-tip <revision>
                          report that we have only objects reachable from this object
    --[no-]negotiate-only do not fetch a packfile; instead, print ancestors of negotiation tips
    --[no-]filter <args>  object filtering
    --[no-]auto-maintenance
                          run 'maintenance --auto' after fetching
    --[no-]auto-gc        run 'maintenance --auto' after fetching
    --[no-]show-forced-updates
                          check for forced-updates on all updated branches
    --[no-]write-commit-graph
                          write the commit-graph after fetching
    --[no-]stdin          accept refspecs from stdin


prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu ((00a7967d...))
$ git fetch b2b-demo-gpu-prod-release-20260121-1
fatal: 'b2b-demo-gpu-prod-release-20260121-1' does not appear to be a git repository
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu ((00a7967d...))
$ git switch b2b-demo-gpu-prod-release-20260121-1
fatal: a branch is expected, got tag 'b2b-demo-gpu-prod-release-20260121-1'
hint: If you want to detach HEAD at the commit, try again with the --detach option.

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu ((00a7967d...))
$ git checkout b2b-demo-gpu-prod-release-20260121-1
Warning: you are leaving 1 commit behind, not connected to
any of your branches:

  00a7967d add backend.tf

If you want to keep it by creating a new branch, this may be a good time
to do so with:

 git branch <new-branch-name> 00a7967d

HEAD is now at c337cd22 Merge pull request #881 from sbintuitions/toura-release-news

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu ((b2b-demo-gpu-prod-release-20260121-1))
$ git log
commit c337cd2210d43520e0fbdd8335c51a19631744bb (HEAD, tag: b2b-demo-gpu-prod-release-20260121-1)
Merge: d24c4b84 7e01507b
Author: Takeshi Oura <takoika@users.noreply.github.com>
Date:   Tue Jan 20 15:44:08 2026 +0900

    Merge pull request #881 from sbintuitions/toura-release-news

    共同通信ニュースとソフトバンクニュースをsbkk demo環境でリリースする

commit d24c4b84de6d0c9cc6106f14071b699d4a7cfb0e
Author: tatsuya-hayashino <tatsuya.hayashino@sbintuitions.co.jp>
Date:   Tue Jan 20 14:51:05 2026 +0900

    Add/Update API keys (#879)

commit cac7129a6f6d7259871facf52aeed91cc912b957
Merge: 931929ca 103c3aab
Author: Takeshi Oura <takoika@users.noreply.github.com>
Date:   Tue Jan 20 13:43:00 2026 +0900

    Merge pull request #880 from sbintuitions/toura-remove-old-vars

    デプロイ時のエラーを避けるために後方互換のために残していたenv varの削除

commit 103c3aab20b8a9337ccd13c3fef1af483ca59cab
Author: takoika <16839037+takoika@users.noreply.github.com>
Date:   Tue Jan 20 13:32:09 2026 +0900

    Remove unnecessary legacy env var

commit 86a0a6a83da00f7bf9200fb562554bbf398e8192
Merge: 27ade62d 931929ca
Author: takoika <16839037+takoika@users.noreply.github.com>
Date:   Tue Jan 20 13:31:32 2026 +0900

    Merge branch 'main' of github.com:sbintuitions/reimei_RAG into toura-remove-old-vars

commit 7e01507b6f44d97f57094369c1f010df799510bb
Author: takoika <16839037+takoika@users.noreply.github.com>

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu ((b2b-demo-gpu-prod-release-20260121-1))
$ git branch
* (HEAD detached at b2b-demo-gpu-prod-release-20260121-1)

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu ((b2b-demo-gpu-prod-release-20260121-1))
$ cd prod/

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((b2b-demo-gpu-prod-release-20260121-1))
$ ls -l
total 83
-rw-r--r-- 1 prod.shogo.kagami 1049089   777 Jan 21 13:15 data.tf
-rw-r--r-- 1 prod.shogo.kagami 1049089  5368 Jan 21 13:15 main.tf
-rw-r--r-- 1 prod.shogo.kagami 1049089   697 Jan 21 13:15 providers.tf
-rw-r--r-- 1 prod.shogo.kagami 1049089     0 Jan 21 13:26 terraform.tfstate
-rw-r--r-- 1 prod.shogo.kagami 1049089 64659 Jan 21 13:26 terraform.tfstate.backup
-rw-r--r-- 1 prod.shogo.kagami 1049089   100 Jan 21 13:15 terraform.tfvars
-rw-r--r-- 1 prod.shogo.kagami 1049089   393 Jan 21 13:15 variables.tf
-rw-r--r-- 1 prod.shogo.kagami 1049089   314 Jan 21 13:15 versions.tf

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((b2b-demo-gpu-prod-release-20260121-1))
$ git checkout 00a7967d
Previous HEAD position was c337cd22 Merge pull request #881 from sbintuitions/toura-release-news
HEAD is now at 00a7967d add backend.tf

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ ls -l
total 84
-rw-r--r-- 1 prod.shogo.kagami 1049089   219 Jan 21 14:26 backend.tf
-rw-r--r-- 1 prod.shogo.kagami 1049089   777 Jan 21 13:15 data.tf
-rw-r--r-- 1 prod.shogo.kagami 1049089  5368 Jan 21 13:15 main.tf
-rw-r--r-- 1 prod.shogo.kagami 1049089   697 Jan 21 13:15 providers.tf
-rw-r--r-- 1 prod.shogo.kagami 1049089     0 Jan 21 13:26 terraform.tfstate
-rw-r--r-- 1 prod.shogo.kagami 1049089 64659 Jan 21 13:26 terraform.tfstate.backup
-rw-r--r-- 1 prod.shogo.kagami 1049089   100 Jan 21 13:15 terraform.tfvars
-rw-r--r-- 1 prod.shogo.kagami 1049089   393 Jan 21 13:15 variables.tf
-rw-r--r-- 1 prod.shogo.kagami 1049089   314 Jan 21 13:15 versions.tf

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git status
HEAD detached at 00a7967d
nothing to commit, working tree clean

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git push origin b2b-demo-gpu-prod-release-20260121-1
Everything up-to-date

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git push origin main
error: src refspec main does not match any
error: failed to push some refs to 'https://github.com/sbintuitions/reimei_RAG'

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git push https://github.com/sbintuitions/reimei_RAG
fatal: You are not currently on a branch.
To push the history leading to the current (detached HEAD)
state now, use

    git push https://github.com/sbintuitions/reimei_RAG HEAD:<name-of-remote-branch>


prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git push -f origin main
error: src refspec main does not match any
error: failed to push some refs to 'https://github.com/sbintuitions/reimei_RAG'

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git log
commit 00a7967d2eb47a58c7ed889c0a859138ec7b23c4 (HEAD)
Author: Shogo Kagami <prod.shogo.kagami@L4VSOC.SBINT.LOCAL>
Date:   Wed Jan 21 13:26:48 2026 +0900

    add backend.tf

commit c337cd2210d43520e0fbdd8335c51a19631744bb (tag: b2b-demo-gpu-prod-release-20260121-1)
Merge: d24c4b84 7e01507b
Author: Takeshi Oura <takoika@users.noreply.github.com>
Date:   Tue Jan 20 15:44:08 2026 +0900

    Merge pull request #881 from sbintuitions/toura-release-news

    共同通信ニュースとソフトバンクニュースをsbkk demo環境でリリースする

commit d24c4b84de6d0c9cc6106f14071b699d4a7cfb0e
Author: tatsuya-hayashino <tatsuya.hayashino@sbintuitions.co.jp>
Date:   Tue Jan 20 14:51:05 2026 +0900

    Add/Update API keys (#879)

commit cac7129a6f6d7259871facf52aeed91cc912b957
Merge: 931929ca 103c3aab
Author: Takeshi Oura <takoika@users.noreply.github.com>
Date:   Tue Jan 20 13:43:00 2026 +0900

    Merge pull request #880 from sbintuitions/toura-remove-old-vars

    デプロイ時のエラーを避けるために後方互換のために残していたenv varの削除

commit 103c3aab20b8a9337ccd13c3fef1af483ca59cab
Author: takoika <16839037+takoika@users.noreply.github.com>
Date:   Tue Jan 20 13:32:09 2026 +0900

    Remove unnecessary legacy env var

commit 86a0a6a83da00f7bf9200fb562554bbf398e8192
Merge: 27ade62d 931929ca
Author: takoika <16839037+takoika@users.noreply.github.com>

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git status
HEAD detached at 00a7967d
nothing to commit, working tree clean

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git branch
* (HEAD detached at 00a7967d)

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git pull
remote: Enumerating objects: 13, done.
remote: Counting objects: 100% (13/13), done.
remote: Compressing objects: 100% (3/3), done.
remote: Total 13 (delta 11), reused 11 (delta 10), pack-reused 0 (from 0)
Unpacking objects: 100% (13/13), 1.93 KiB | 21.00 KiB/s, done.
From https://github.com/sbintuitions/reimei_RAG
   43c70825..33be7684  main               -> origin/main
   2d3d45f8..014308e4  toura-event-bridge -> origin/toura-event-bridge
 * [new tag]           demo-prod-release-20260121-1 -> demo-prod-release-20260121-1
You are not currently on a branch.
Please specify which branch you want to merge with.
See git-pull(1) for details.

    git pull <remote> <branch>


prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git push origin main
error: src refspec main does not match any
error: failed to push some refs to 'https://github.com/sbintuitions/reimei_RAG'

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git branch
* (HEAD detached at 00a7967d)

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git status
HEAD detached at 00a7967d
nothing to commit, working tree clean

prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$ git push
fatal: You are not currently on a branch.
To push the history leading to the current (detached HEAD)
state now, use

    git push origin HEAD:<name-of-remote-branch>


prod.shogo.kagami@VSOC-DESKTOP01 MINGW64 ~/deploy_20260121/reimei_RAG/terraform/environments_b2b_demo_gpu/prod ((00a7967d...))
$
